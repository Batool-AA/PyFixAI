{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11150639,"sourceType":"datasetVersion","datasetId":6956758},{"sourceId":11259905,"sourceType":"datasetVersion","datasetId":7037387},{"sourceId":11350045,"sourceType":"datasetVersion","datasetId":7101948},{"sourceId":11350256,"sourceType":"datasetVersion","datasetId":7102129},{"sourceId":11398360,"sourceType":"datasetVersion","datasetId":7138777},{"sourceId":11415698,"sourceType":"datasetVersion","datasetId":7149610}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\n\ndef load_and_preprocess_data(file_path):\n    \"\"\"Loads and preprocesses the CodeNet dataset for training.\"\"\"\n    \n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)  # Load all data at once\n\n    preprocessed_data = []\n\n    for i, entry in enumerate(data):\n        \n            preprocessed_data.append(entry)\n\n    return preprocessed_data\n\n# Replace with actual path\ntrain_file = \"/kaggle/input/code-net-python/train.jsonl\"\ntrain_data = load_and_preprocess_data(train_file)\nprint(train_data[0])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:05:38.929457Z","iopub.execute_input":"2025-04-15T08:05:38.930190Z","iopub.status.idle":"2025-04-15T08:06:16.706037Z","shell.execute_reply.started":"2025-04-15T08:05:38.930159Z","shell.execute_reply":"2025-04-15T08:06:16.705363Z"}},"outputs":[{"name":"stdout","text":"{'src_id': 'p00001_s631177546', 'src': ['from', 'sys', 'import', 'stdin', 'NEW_LINE', 'x', '=', '[', 'int', '(', 'input', '(', ')', ')', 'for', 'i', 'in', 'range', '(', '10', ')', ']', 'NEW_LINE', 'x', '.', 'reverse', '(', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '3', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'i', ')', 'NEW_LINE', 'DEDENT'], 'src_verdict': 'Wrong Answer', 'tgt': ['from', 'sys', 'import', 'stdin', 'NEW_LINE', 'x', '=', '[', 'int', '(', 'input', '(', ')', ')', 'for', 'i', 'in', 'range', '(', '10', ')', ']', 'NEW_LINE', 'x', '.', 'sort', '(', 'reverse', '=', 'True', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '3', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'x', '[', 'i', ']', ')', 'NEW_LINE', 'DEDENT'], 'tgt_id': 'p00001_s854661751'}\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install torch transformers\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:06:20.863652Z","iopub.execute_input":"2025-04-15T08:06:20.864274Z","iopub.status.idle":"2025-04-15T08:07:35.618834Z","shell.execute_reply.started":"2025-04-15T08:06:20.864253Z","shell.execute_reply":"2025-04-15T08:07:35.618107Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import RobertaTokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:08:08.392573Z","iopub.execute_input":"2025-04-15T08:08:08.392882Z","iopub.status.idle":"2025-04-15T08:08:21.421575Z","shell.execute_reply.started":"2025-04-15T08:08:08.392857Z","shell.execute_reply":"2025-04-15T08:08:21.420999Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a402293aad9943cc876550e31fbb2fa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"166d313d5c2f40978180916298d17fe6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f5f199205da4780b804064ed9be3f49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe209dbec7c8453ca237a73015f7a71d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f1919b496394438a1f34b9fbeda5326"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"validation_file = \"/kaggle/input/code-net-python/valid.jsonl\"\nvalidation_data = load_and_preprocess_data(validation_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:08:25.022567Z","iopub.execute_input":"2025-04-15T08:08:25.022968Z","iopub.status.idle":"2025-04-15T08:08:43.878297Z","shell.execute_reply.started":"2025-04-15T08:08:25.022946Z","shell.execute_reply":"2025-04-15T08:08:43.877741Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def get_token_lengths(data, tokenizer):\n    src_lengths = []\n    tgt_lengths = []\n    for example in data:\n        src_text = \" \".join(example['src'])\n        tgt_text = \" \".join(example['tgt'])\n\n        src_enc = tokenizer(src_text, truncation=False)['input_ids']\n        tgt_enc = tokenizer(tgt_text, truncation=False)['input_ids']\n\n        src_lengths.append(len(src_enc))\n        tgt_lengths.append(len(tgt_enc))\n    \n    return src_lengths, tgt_lengths\n\nimport numpy as np\nsrc_lengths,tgt_lengths = get_token_lengths(train_data[50000:80000],tokenizer)\n\nprint(\"Source Lengths:\")\nprint(f\"Mean: {np.mean(src_lengths):.2f}, 90th percentile: {np.percentile(src_lengths, 90)}, Max: {max(src_lengths)}\")\n\nprint(\"\\nTarget Lengths:\")\nprint(f\"Mean: {np.mean(tgt_lengths):.2f}, 90th percentile: {np.percentile(tgt_lengths, 90)}, Max: {max(tgt_lengths)}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(tokenizer.model_max_length)  # This will print 512\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\ndef encode_example(example, tokenizer, max_length=512):\n    src_tokens = example['src']\n    tgt_tokens = example['tgt']\n    src_text = \" \".join(src_tokens)\n    \n    # Add start and end tokens to the target\n    tgt_text = \"<s> \" + \" \".join(tgt_tokens) + \" </s>\"\n    \n    src_enc = tokenizer(src_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n    tgt_enc = tokenizer(tgt_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n    return src_enc, tgt_enc\n\n\n\nclass PreTokenizedDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.tokenized_data = tokenized_data\n    \n    def __len__(self):\n        return len(self.tokenized_data)\n    \n    def __getitem__(self, idx):\n        return self.tokenized_data[idx]\n\ndef pre_tokenize_data(data, tokenizer, max_length=512):\n    tokenized_data = []\n    for example in data:\n        src_enc, tgt_enc = encode_example(example, tokenizer, max_length)\n        tokenized_data.append({\n            'src_input_ids': src_enc['input_ids'].squeeze(0),\n            'src_attention_mask': src_enc['attention_mask'].squeeze(0),\n            'tgt_input_ids': tgt_enc['input_ids'].squeeze(0),\n            'tgt_attention_mask': tgt_enc['attention_mask'].squeeze(0)\n        })\n    return tokenized_data\n\ntokenized_train_data = pre_tokenize_data(train_data[130000:180000], tokenizer, max_length=512)\npretokenized_dataset = PreTokenizedDataset(tokenized_train_data)\ntrain_loader = DataLoader(pretokenized_dataset, batch_size=8, shuffle=True)\n\ntokenized_valid_data = pre_tokenize_data(validation_data[130000:180000], tokenizer, max_length=512)\npretokenized_valid_dataset = PreTokenizedDataset(tokenized_valid_data)\nvalid_loader = DataLoader(pretokenized_valid_dataset, batch_size=8, shuffle=True)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:08:51.645329Z","iopub.execute_input":"2025-04-15T08:08:51.645640Z","iopub.status.idle":"2025-04-15T08:13:49.360929Z","shell.execute_reply.started":"2025-04-15T08:08:51.645618Z","shell.execute_reply":"2025-04-15T08:13:49.360313Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=512):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n        self.register_buffer('pe', pe)\n    \n    def forward(self, x):\n        # x shape: (batch_size, seq_len, d_model)\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)\n\nclass CodeErrorFixModel(nn.Module):\n    def __init__(self, encoder_model_name, vocab_size, embed_size=768, num_decoder_layers=6, nhead=8):\n        super().__init__()\n        # Load the pretrained CodeBERT encoder\n        self.encoder = AutoModel.from_pretrained(encoder_model_name)\n        # Decoder components\n        self.decoder_embedding = nn.Embedding(vocab_size, embed_size)\n        self.pos_encoder = PositionalEncoding(embed_size)\n        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dropout=0.1)\n        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n        self.fc_out = nn.Linear(embed_size, vocab_size)\n        self.embed_size = embed_size\n    \n    def generate_square_subsequent_mask(self, sz):\n        # Create a mask to ensure that each position only attends to previous positions\n        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n        return mask.to(next(self.parameters()).device)\n    \n    def forward(self, src_input_ids, src_attention_mask, tgt_input_ids, tgt_attention_mask):\n        # Encode source sequence\n        encoder_outputs = self.encoder(input_ids=src_input_ids, attention_mask=src_attention_mask)\n        memory = encoder_outputs.last_hidden_state  # shape: (batch_size, src_seq_len, embed_size)\n        \n        # Prepare target embeddings\n        tgt_embeddings = self.decoder_embedding(tgt_input_ids) * math.sqrt(self.embed_size)\n        tgt_embeddings = self.pos_encoder(tgt_embeddings)\n        # Transformer expects (seq_len, batch_size, embed_size)\n        tgt_embeddings = tgt_embeddings.transpose(0, 1)\n        memory = memory.transpose(0, 1)\n        \n        tgt_seq_len = tgt_input_ids.size(1)\n        # Create target mask for auto-regressive generation\n        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n        \n        decoder_output = self.decoder(tgt=tgt_embeddings, memory=memory, tgt_mask=tgt_mask)\n        # Transpose back: (batch_size, seq_len, embed_size)\n        decoder_output = decoder_output.transpose(0, 1)\n        logits = self.fc_out(decoder_output)  # (batch_size, seq_len, vocab_size)\n        return logits\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:15:17.197742Z","iopub.execute_input":"2025-04-15T08:15:17.198222Z","iopub.status.idle":"2025-04-15T08:15:17.208033Z","shell.execute_reply.started":"2025-04-15T08:15:17.198197Z","shell.execute_reply":"2025-04-15T08:15:17.207313Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport math\nfrom transformers import AutoTokenizer, AutoModel\n# === Step 1: Load tokenizer from saved folder ===\ntokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/model-3/no3\")\n\n# === Step 2: Load model from .pth ===\nmodel = torch.load(\"/kaggle/input/model-3/PYFIX_MODEL_3/full_model.pth\")\n\n# === Step 3: Send to device ===\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n# We'll ignore the padding tokens when computing loss\npad_token_id = tokenizer.pad_token_id  # Make sure you have a tokenizer object\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:15:20.867801Z","iopub.execute_input":"2025-04-15T08:15:20.868078Z","iopub.status.idle":"2025-04-15T08:15:50.423239Z","shell.execute_reply.started":"2025-04-15T08:15:20.868048Z","shell.execute_reply":"2025-04-15T08:15:50.422516Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/3732261795.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load(\"/kaggle/input/model-3/PYFIX_MODEL_3/full_model.pth\")\n2025-04-15 08:15:32.738200: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744704933.039795      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744704933.122562      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install tqdm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:15:57.579677Z","iopub.execute_input":"2025-04-15T08:15:57.580695Z","iopub.status.idle":"2025-04-15T08:16:01.132003Z","shell.execute_reply.started":"2025-04-15T08:15:57.580662Z","shell.execute_reply":"2025-04-15T08:16:01.131258Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"vocab_size = tokenizer.vocab_size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:16:03.827960Z","iopub.execute_input":"2025-04-15T08:16:03.828257Z","iopub.status.idle":"2025-04-15T08:16:03.832175Z","shell.execute_reply.started":"2025-04-15T08:16:03.828232Z","shell.execute_reply":"2025-04-15T08:16:03.831427Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nnum_epochs = 2\n\nmodel.train()\nprint(\"Training Started\")\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    \n    for i, batch in enumerate(tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}\")):\n        src_input_ids = batch['src_input_ids'].to(device)\n        src_attention_mask = batch['src_attention_mask'].to(device)\n        tgt_input_ids = batch['tgt_input_ids'].to(device)\n        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n        \n        optimizer.zero_grad()\n        \n        decoder_input_ids = tgt_input_ids[:, :-1]\n        decoder_target_ids = tgt_input_ids[:, 1:]\n        \n        logits = model(\n            src_input_ids=src_input_ids,\n            src_attention_mask=src_attention_mask,\n            tgt_input_ids=decoder_input_ids,\n            tgt_attention_mask=tgt_attention_mask[:, :-1]\n        )\n        \n        logits = logits.reshape(-1, vocab_size)\n        decoder_target_ids = decoder_target_ids.reshape(-1)\n        \n        loss = criterion(logits, decoder_target_ids)\n        loss.backward()\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    avg_loss = total_loss / len(train_loader)\n    print(f\"✅ Epoch {epoch+1}/{num_epochs} - Average Training Loss: {avg_loss:.4f}\")\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for batch in valid_loader:\n            src_input_ids = batch['src_input_ids'].to(device)\n            src_attention_mask = batch['src_attention_mask'].to(device)\n            tgt_input_ids = batch['tgt_input_ids'].to(device)\n            tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n\n            decoder_input_ids = tgt_input_ids[:, :-1]\n            decoder_target_ids = tgt_input_ids[:, 1:]\n\n            logits = model(\n                src_input_ids=src_input_ids,\n                src_attention_mask=src_attention_mask,\n                tgt_input_ids=decoder_input_ids,\n                tgt_attention_mask=tgt_attention_mask[:, :-1]\n            )\n\n            logits = logits.reshape(-1, vocab_size)\n            decoder_target_ids = decoder_target_ids.reshape(-1)\n\n            loss = criterion(logits, decoder_target_ids)\n            val_loss += loss.item()\n\n    val_avg_loss = val_loss / len(valid_loader)\n    print(f\"🧪 Epoch {epoch+1} - Validation Loss: {val_avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T08:16:07.952531Z","iopub.execute_input":"2025-04-15T08:16:07.952883Z","iopub.status.idle":"2025-04-15T16:25:51.480506Z","shell.execute_reply.started":"2025-04-15T08:16:07.952861Z","shell.execute_reply":"2025-04-15T16:25:51.479776Z"}},"outputs":[{"name":"stdout","text":"Training Started\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/6250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be8ea09b25e445049a887d9dedb4c31a"}},"metadata":{}},{"name":"stdout","text":"✅ Epoch 1/2 - Average Training Loss: 0.3161\n🧪 Epoch 1 - Validation Loss: 0.5780\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/6250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b66fe42ceb6b46c0a5d6ad89b56311b4"}},"metadata":{}},{"name":"stdout","text":"✅ Epoch 2/2 - Average Training Loss: 0.2055\n🧪 Epoch 2 - Validation Loss: 0.6131\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"torch.save(model, \"full_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:40:17.076488Z","iopub.execute_input":"2025-04-15T16:40:17.076831Z","iopub.status.idle":"2025-04-15T16:40:18.810492Z","shell.execute_reply.started":"2025-04-15T16:40:17.076810Z","shell.execute_reply":"2025-04-15T16:40:18.809741Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!zip -r model_archive.zip full_model.pth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:40:44.283560Z","iopub.execute_input":"2025-04-15T16:40:44.284250Z","iopub.status.idle":"2025-04-15T16:41:36.316590Z","shell.execute_reply.started":"2025-04-15T16:40:44.284228Z","shell.execute_reply":"2025-04-15T16:41:36.315867Z"}},"outputs":[{"name":"stdout","text":"  adding: full_model.pth (deflated 7%)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import json\nimport requests\nfrom google.colab import auth  # works in Kaggle too\nimport google.auth\nfrom google.auth.transport.requests import Request\n\n\n\n\nauth.authenticate_user()\ncreds, _ = google.auth.default()\ncreds.refresh(Request())\naccess_token = creds.token\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:46:16.093967Z","iopub.execute_input":"2025-04-15T16:46:16.094327Z","iopub.status.idle":"2025-04-15T16:46:59.921992Z","shell.execute_reply.started":"2025-04-15T16:46:16.094300Z","shell.execute_reply":"2025-04-15T16:46:59.921433Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"file_path = \"/kaggle/working/model_archive.zip\"  # Change this\nfile_name = \"PYFIX_MODEL_4.zip\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {access_token}\"\n}\n\nmetadata = {\n    \"name\": file_name,\n    \"mimeType\": \"application/zip\"\n}\n\nfiles = {\n    \"data\": (\"metadata\", json.dumps(metadata), \"application/json\"),\n    \"file\": open(file_path, \"rb\")\n}\n\nupload_url = \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\"\n\nres = requests.post(upload_url, headers=headers, files=files)\nres.raise_for_status()\n\nprint(\"✅ Upload successful!\")\nprint(\"📁 File ID:\", res.json()[\"id\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:47:05.863804Z","iopub.execute_input":"2025-04-15T16:47:05.864092Z","iopub.status.idle":"2025-04-15T16:47:16.395424Z","shell.execute_reply.started":"2025-04-15T16:47:05.864073Z","shell.execute_reply":"2025-04-15T16:47:16.394819Z"}},"outputs":[{"name":"stdout","text":"✅ Upload successful!\n📁 File ID: 1es-vZJT3ftdUoh5FfeVTDUFbL9vm-UzP\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"tokenizer.save_pretrained(\"tokenizer_dir\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:47:41.652361Z","iopub.execute_input":"2025-04-15T16:47:41.652958Z","iopub.status.idle":"2025-04-15T16:47:41.742160Z","shell.execute_reply.started":"2025-04-15T16:47:41.652936Z","shell.execute_reply":"2025-04-15T16:47:41.741357Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('tokenizer_dir/tokenizer_config.json',\n 'tokenizer_dir/special_tokens_map.json',\n 'tokenizer_dir/vocab.json',\n 'tokenizer_dir/merges.txt',\n 'tokenizer_dir/added_tokens.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"tokenizer_dir\", 'zip', \"tokenizer_dir\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:47:44.934677Z","iopub.execute_input":"2025-04-15T16:47:44.935274Z","iopub.status.idle":"2025-04-15T16:47:45.040159Z","shell.execute_reply.started":"2025-04-15T16:47:44.935252Z","shell.execute_reply":"2025-04-15T16:47:45.039561Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/tokenizer_dir.zip'"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer  # or your specific tokenizer\n\n# Load model\nmodel = torch.load(\"full_model.pth\", map_location=torch.device(\"cpu\"))\nmodel.eval()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"tokenizer_dir\")\n\n# Example usage\nsrc_code = \"def add(x, y): return x + y\"\ntokens = tokenizer(src_code, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n\n# Inference\nwith torch.no_grad():\n    output = model(tokens[\"input_ids\"], tokens[\"attention_mask\"], ...)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_file = \"/kaggle/input/code-net-python/test.jsonl\"\ntest_data = load_and_preprocess_data(test_file)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:49:48.303396Z","iopub.execute_input":"2025-04-15T16:49:48.303693Z","iopub.status.idle":"2025-04-15T16:50:13.199043Z","shell.execute_reply.started":"2025-04-15T16:49:48.303673Z","shell.execute_reply":"2025-04-15T16:50:13.198431Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import re\n\ndef clean_prediction(text):\n    # Remove common garbage patterns\n    text = re.sub(r\"(=|NEW|\\\\n|\\+|\\s){5,}\", \"\", text)\n    text = re.sub(r\"[^a-zA-Z0-9\\s\\.\\,\\:\\(\\)\\[\\]\\+\\-\\*/\\<\\>=\\'\\\"\\_]+\", \"\", text)  # remove weird tokens\n    text = text.strip()\n    return text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:56:20.804872Z","iopub.execute_input":"2025-04-15T16:56:20.805174Z","iopub.status.idle":"2025-04-15T16:56:20.809671Z","shell.execute_reply.started":"2025-04-15T16:56:20.805153Z","shell.execute_reply":"2025-04-15T16:56:20.808910Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaTokenizer\nfrom torch.utils.data import DataLoader\n\n# 1. Load your saved model\nmodel = torch.load('/kaggle/input/model-pyfix/PYFIX_MODEL/full_model.pth')  \nmodel.eval()  # Set the model to evaluation mode\n\n# 2. Load the tokenizer\ntokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n\n# 3. Prepare test data\ndata_test = test_data[:1]  # or use more examples if needed\n\n# Tokenize the test data\ntokenized_test_data = pre_tokenize_data(data_test, tokenizer, max_length=512)\n\n# Create DataLoader\ntest_loader = DataLoader(tokenized_test_data, batch_size=1)  # batch_size=1 for clarity\n\n# 4. Run inference\nwith torch.no_grad():\n    for batch in test_loader:\n        src_input_ids = batch['src_input_ids'].to(device)\n        src_attention_mask = batch['src_attention_mask'].to(device)\n        tgt_input_ids = batch['tgt_input_ids'].to(device)\n        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n\n        # 🔮 Predict (use teacher forcing)\n        output = model(src_input_ids, src_attention_mask, tgt_input_ids[:, :-1], tgt_attention_mask[:, :-1])\n        predicted_ids = output.argmax(dim=-1)\n\n        # Decode Input (buggy), Prediction, and Target (ground truth)\n        input_text = tokenizer.decode(src_input_ids[0], skip_special_tokens=True)\n        predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n        target_text = tokenizer.decode(tgt_input_ids[0], skip_special_tokens=True)\n        predicted_text = clean_prediction(predicted_text)\n\n        print(\"🧪 Input (Buggy Code):\")\n        print(input_text)\n        print(\"\\n✅ Prediction (Model Fix):\")\n        print(predicted_text)\n        print(\"\\n🎯 Target (Ground Truth Fix):\")\n        print(target_text)\n        print(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:56:50.554359Z","iopub.execute_input":"2025-04-15T16:56:50.555057Z","iopub.status.idle":"2025-04-15T16:56:51.722000Z","shell.execute_reply.started":"2025-04-15T16:56:50.555033Z","shell.execute_reply":"2025-04-15T16:56:51.721211Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/279323738.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model = torch.load('/kaggle/input/model-pyfix/PYFIX_MODEL/full_model.pth')\n","output_type":"stream"},{"name":"stdout","text":"🧪 Input (Buggy Code):\nN = int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( 1 , N - 1 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 1 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT\n\n✅ Prediction (Model Fix):\n= int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( 1 ) 1 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 2 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT\n\n🎯 Target (Ground Truth Fix):\n N = int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( N - 2 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 1 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT \n================================================================================\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(test_data[9])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T18:46:35.816009Z","iopub.execute_input":"2025-04-13T18:46:35.816286Z","iopub.status.idle":"2025-04-13T18:46:35.820527Z","shell.execute_reply.started":"2025-04-13T18:46:35.816267Z","shell.execute_reply":"2025-04-13T18:46:35.819845Z"}},"outputs":[{"name":"stdout","text":"{'src_id': 'p02971_s801719045', 'src': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '=', '[', ']', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'a', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '.', 'append', '(', 'a', ')', 'NEW_LINE', 'DEDENT', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'if', 'A', '[', 'i', ']', '!=', 'max', '(', 'A', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'max', '(', 'A', ')', ')', 'NEW_LINE', 'DEDENT', 'else', ':', 'NEW_LINE', 'INDENT', 'p', '=', 'A', '[', 'i', ']', 'NEW_LINE', 'A', '[', 'i', ']', '=', '0', 'NEW_LINE', 'print', '(', 'max', '(', 'A', ')', ')', 'NEW_LINE', 'A', '[', 'i', ']', '=', 'p', 'NEW_LINE', 'DEDENT', 'DEDENT'], 'src_verdict': 'Time Limit Exceeded', 'tgt': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '=', '[', ']', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'a', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '.', 'append', '(', 'a', ')', 'NEW_LINE', 'DEDENT', 'AA', '=', 'sorted', '(', 'A', ',', 'reverse', '=', 'True', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'if', 'A', '[', 'i', ']', '!=', 'AA', '[', '0', ']', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'AA', '[', '0', ']', ')', 'NEW_LINE', 'DEDENT', 'else', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'AA', '[', '1', ']', ')', 'NEW_LINE', 'DEDENT', 'DEDENT'], 'tgt_id': 'p02971_s747975884'}\n","output_type":"stream"}],"execution_count":29}]}