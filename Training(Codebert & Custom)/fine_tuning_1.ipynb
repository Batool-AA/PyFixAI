{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T11:28:46.210087Z",
     "iopub.status.busy": "2025-04-13T11:28:46.209351Z",
     "iopub.status.idle": "2025-04-13T11:29:23.874066Z",
     "shell.execute_reply": "2025-04-13T11:29:23.873307Z",
     "shell.execute_reply.started": "2025-04-13T11:28:46.210061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_id': 'p00001_s631177546', 'src': ['from', 'sys', 'import', 'stdin', 'NEW_LINE', 'x', '=', '[', 'int', '(', 'input', '(', ')', ')', 'for', 'i', 'in', 'range', '(', '10', ')', ']', 'NEW_LINE', 'x', '.', 'reverse', '(', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '3', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'i', ')', 'NEW_LINE', 'DEDENT'], 'src_verdict': 'Wrong Answer', 'tgt': ['from', 'sys', 'import', 'stdin', 'NEW_LINE', 'x', '=', '[', 'int', '(', 'input', '(', ')', ')', 'for', 'i', 'in', 'range', '(', '10', ')', ']', 'NEW_LINE', 'x', '.', 'sort', '(', 'reverse', '=', 'True', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '3', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'x', '[', 'i', ']', ')', 'NEW_LINE', 'DEDENT'], 'tgt_id': 'p00001_s854661751'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the CodeNet dataset for training.\"\"\"\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # Load all data at once\n",
    "\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        \n",
    "            preprocessed_data.append(entry)\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "# Replace with actual path\n",
    "train_file = \"/kaggle/input/code-net/train.jsonl\"\n",
    "train_data = load_and_preprocess_data(train_file)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:29:30.737693Z",
     "iopub.status.busy": "2025-04-13T11:29:30.737120Z",
     "iopub.status.idle": "2025-04-13T11:31:00.074976Z",
     "shell.execute_reply": "2025-04-13T11:31:00.074153Z",
     "shell.execute_reply.started": "2025-04-13T11:29:30.737653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:31:20.732625Z",
     "iopub.status.busy": "2025-04-13T11:31:20.732276Z",
     "iopub.status.idle": "2025-04-13T11:31:37.646502Z",
     "shell.execute_reply": "2025-04-13T11:31:37.645940Z",
     "shell.execute_reply.started": "2025-04-13T11:31:20.732600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7899ec4c2f574950aefe4d9764613a53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0694347c6824bd09758bd09269a1509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1061bb2a8b4f2097997a6484579ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ea8b1478234722bf4df8a9b71c4958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cd0c817710435aa42606a353fa5021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:31:42.275420Z",
     "iopub.status.busy": "2025-04-13T11:31:42.274555Z",
     "iopub.status.idle": "2025-04-13T11:32:01.301481Z",
     "shell.execute_reply": "2025-04-13T11:32:01.300919Z",
     "shell.execute_reply.started": "2025-04-13T11:31:42.275389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "validation_file = \"/kaggle/input/code-net/valid.jsonl\"\n",
    "validation_data = load_and_preprocess_data(validation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_token_lengths(data, tokenizer):\n",
    "    src_lengths = []\n",
    "    tgt_lengths = []\n",
    "    for example in data:\n",
    "        src_text = \" \".join(example['src'])\n",
    "        tgt_text = \" \".join(example['tgt'])\n",
    "\n",
    "        src_enc = tokenizer(src_text, truncation=False)['input_ids']\n",
    "        tgt_enc = tokenizer(tgt_text, truncation=False)['input_ids']\n",
    "\n",
    "        src_lengths.append(len(src_enc))\n",
    "        tgt_lengths.append(len(tgt_enc))\n",
    "    \n",
    "    return src_lengths, tgt_lengths\n",
    "\n",
    "import numpy as np\n",
    "src_lengths,tgt_lengths = get_token_lengths(train_data[50000:80000],tokenizer)\n",
    "\n",
    "print(\"Source Lengths:\")\n",
    "print(f\"Mean: {np.mean(src_lengths):.2f}, 90th percentile: {np.percentile(src_lengths, 90)}, Max: {max(src_lengths)}\")\n",
    "\n",
    "print(\"\\nTarget Lengths:\")\n",
    "print(f\"Mean: {np.mean(tgt_lengths):.2f}, 90th percentile: {np.percentile(tgt_lengths, 90)}, Max: {max(tgt_lengths)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.model_max_length)  # This will print 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:32:08.719342Z",
     "iopub.status.busy": "2025-04-13T11:32:08.719031Z",
     "iopub.status.idle": "2025-04-13T11:35:03.797946Z",
     "shell.execute_reply": "2025-04-13T11:35:03.797300Z",
     "shell.execute_reply.started": "2025-04-13T11:32:08.719321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def encode_example(example, tokenizer, max_length=512):\n",
    "    src_tokens = example['src']\n",
    "    tgt_tokens = example['tgt']\n",
    "    src_text = \" \".join(src_tokens)\n",
    "    \n",
    "    # Add start and end tokens to the target\n",
    "    tgt_text = \"<s> \" + \" \".join(tgt_tokens) + \" </s>\"\n",
    "    \n",
    "    src_enc = tokenizer(src_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    tgt_enc = tokenizer(tgt_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return src_enc, tgt_enc\n",
    "\n",
    "\n",
    "\n",
    "class PreTokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_data[idx]\n",
    "\n",
    "def pre_tokenize_data(data, tokenizer, max_length=512):\n",
    "    tokenized_data = []\n",
    "    for example in data:\n",
    "        src_enc, tgt_enc = encode_example(example, tokenizer, max_length)\n",
    "        tokenized_data.append({\n",
    "            'src_input_ids': src_enc['input_ids'].squeeze(0),\n",
    "            'src_attention_mask': src_enc['attention_mask'].squeeze(0),\n",
    "            'tgt_input_ids': tgt_enc['input_ids'].squeeze(0),\n",
    "            'tgt_attention_mask': tgt_enc['attention_mask'].squeeze(0)\n",
    "        })\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_train_data = pre_tokenize_data(train_data[80000:130000], tokenizer, max_length=512)\n",
    "pretokenized_dataset = PreTokenizedDataset(tokenized_train_data)\n",
    "train_loader = DataLoader(pretokenized_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "tokenized_valid_data = pre_tokenize_data(validation_data[80000:130000], tokenizer, max_length=512)\n",
    "pretokenized_valid_dataset = PreTokenizedDataset(tokenized_valid_data)\n",
    "valid_loader = DataLoader(pretokenized_valid_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:35:16.846338Z",
     "iopub.status.busy": "2025-04-13T11:35:16.846059Z",
     "iopub.status.idle": "2025-04-13T11:35:16.856161Z",
     "shell.execute_reply": "2025-04-13T11:35:16.855385Z",
     "shell.execute_reply.started": "2025-04-13T11:35:16.846317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CodeErrorFixModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name, vocab_size, embed_size=768, num_decoder_layers=6, nhead=8):\n",
    "        super().__init__()\n",
    "        # Load the pretrained CodeBERT encoder\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model_name)\n",
    "        # Decoder components\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dropout=0.1)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Create a mask to ensure that each position only attends to previous positions\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask.to(next(self.parameters()).device)\n",
    "    \n",
    "    def forward(self, src_input_ids, src_attention_mask, tgt_input_ids, tgt_attention_mask):\n",
    "        # Encode source sequence\n",
    "        encoder_outputs = self.encoder(input_ids=src_input_ids, attention_mask=src_attention_mask)\n",
    "        memory = encoder_outputs.last_hidden_state  # shape: (batch_size, src_seq_len, embed_size)\n",
    "        \n",
    "        # Prepare target embeddings\n",
    "        tgt_embeddings = self.decoder_embedding(tgt_input_ids) * math.sqrt(self.embed_size)\n",
    "        tgt_embeddings = self.pos_encoder(tgt_embeddings)\n",
    "        # Transformer expects (seq_len, batch_size, embed_size)\n",
    "        tgt_embeddings = tgt_embeddings.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        tgt_seq_len = tgt_input_ids.size(1)\n",
    "        # Create target mask for auto-regressive generation\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        \n",
    "        decoder_output = self.decoder(tgt=tgt_embeddings, memory=memory, tgt_mask=tgt_mask)\n",
    "        # Transpose back: (batch_size, seq_len, embed_size)\n",
    "        decoder_output = decoder_output.transpose(0, 1)\n",
    "        logits = self.fc_out(decoder_output)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:35:22.828339Z",
     "iopub.status.busy": "2025-04-13T11:35:22.827858Z",
     "iopub.status.idle": "2025-04-13T11:36:00.953980Z",
     "shell.execute_reply": "2025-04-13T11:36:00.953148Z",
     "shell.execute_reply.started": "2025-04-13T11:35:22.828315Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3969302437.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"/kaggle/input/model-pyfix/PYFIX_MODEL/full_model.pth\")\n",
      "2025-04-13 11:35:31.664830: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744544132.117016      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744544132.236995      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# === Step 1: Load tokenizer from saved folder ===\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/model-pyfix/tokenizer_dir\")\n",
    "\n",
    "# === Step 2: Load model from .pth ===\n",
    "model = torch.load(\"/kaggle/input/model-pyfix/PYFIX_MODEL/full_model.pth\")\n",
    "\n",
    "# === Step 3: Send to device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "# We'll ignore the padding tokens when computing loss\n",
    "pad_token_id = tokenizer.pad_token_id  # Make sure you have a tokenizer object\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:36:06.714278Z",
     "iopub.status.busy": "2025-04-13T11:36:06.713220Z",
     "iopub.status.idle": "2025-04-13T11:36:10.567532Z",
     "shell.execute_reply": "2025-04-13T11:36:10.566694Z",
     "shell.execute_reply.started": "2025-04-13T11:36:06.714232Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:36:14.929892Z",
     "iopub.status.busy": "2025-04-13T11:36:14.929567Z",
     "iopub.status.idle": "2025-04-13T11:36:14.934811Z",
     "shell.execute_reply": "2025-04-13T11:36:14.934042Z",
     "shell.execute_reply.started": "2025-04-13T11:36:14.929865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:36:19.194458Z",
     "iopub.status.busy": "2025-04-13T11:36:19.194204Z",
     "iopub.status.idle": "2025-04-13T18:17:05.473339Z",
     "shell.execute_reply": "2025-04-13T18:17:05.472314Z",
     "shell.execute_reply.started": "2025-04-13T11:36:19.194442Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b4e99320c342c4a0cad635f50b54f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/3 - Average Training Loss: 0.4266\n",
      "🧪 Epoch 1 - Validation Loss: 0.7361\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7df3cea1ca04a8da89ff00650b2b548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2/3 - Average Training Loss: 0.2719\n",
      "🧪 Epoch 2 - Validation Loss: 0.7507\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a4572669314a49b069322a6aa18a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/3750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3/3 - Average Training Loss: 0.2089\n",
      "🧪 Epoch 3 - Validation Loss: 0.7664\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "print(\"Training Started\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}\")):\n",
    "        src_input_ids = batch['src_input_ids'].to(device)\n",
    "        src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "        tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoder_input_ids = tgt_input_ids[:, :-1]\n",
    "        decoder_target_ids = tgt_input_ids[:, 1:]\n",
    "        \n",
    "        logits = model(\n",
    "            src_input_ids=src_input_ids,\n",
    "            src_attention_mask=src_attention_mask,\n",
    "            tgt_input_ids=decoder_input_ids,\n",
    "            tgt_attention_mask=tgt_attention_mask[:, :-1]\n",
    "        )\n",
    "        \n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        decoder_target_ids = decoder_target_ids.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits, decoder_target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1}/{num_epochs} - Average Training Loss: {avg_loss:.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            src_input_ids = batch['src_input_ids'].to(device)\n",
    "            src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "            tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "            tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "            decoder_input_ids = tgt_input_ids[:, :-1]\n",
    "            decoder_target_ids = tgt_input_ids[:, 1:]\n",
    "\n",
    "            logits = model(\n",
    "                src_input_ids=src_input_ids,\n",
    "                src_attention_mask=src_attention_mask,\n",
    "                tgt_input_ids=decoder_input_ids,\n",
    "                tgt_attention_mask=tgt_attention_mask[:, :-1]\n",
    "            )\n",
    "\n",
    "            logits = logits.reshape(-1, vocab_size)\n",
    "            decoder_target_ids = decoder_target_ids.reshape(-1)\n",
    "\n",
    "            loss = criterion(logits, decoder_target_ids)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_avg_loss = val_loss / len(valid_loader)\n",
    "    print(f\"🧪 Epoch {epoch+1} - Validation Loss: {val_avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:31:04.480086Z",
     "iopub.status.busy": "2025-04-13T18:31:04.479558Z",
     "iopub.status.idle": "2025-04-13T18:31:06.138248Z",
     "shell.execute_reply": "2025-04-13T18:31:06.137612Z",
     "shell.execute_reply.started": "2025-04-13T18:31:04.480064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"full_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:37:52.202124Z",
     "iopub.status.busy": "2025-04-13T18:37:52.201807Z",
     "iopub.status.idle": "2025-04-13T18:38:46.820325Z",
     "shell.execute_reply": "2025-04-13T18:38:46.819483Z",
     "shell.execute_reply.started": "2025-04-13T18:37:52.202091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: full_model.pth (deflated 7%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r model_archive.zip full_model.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:39:14.435034Z",
     "iopub.status.busy": "2025-04-13T18:39:14.434350Z",
     "iopub.status.idle": "2025-04-13T18:40:09.872586Z",
     "shell.execute_reply": "2025-04-13T18:40:09.871895Z",
     "shell.execute_reply.started": "2025-04-13T18:39:14.435004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from google.colab import auth  # works in Kaggle too\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "creds.refresh(Request())\n",
    "access_token = creds.token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:40:13.351945Z",
     "iopub.status.busy": "2025-04-13T18:40:13.351197Z",
     "iopub.status.idle": "2025-04-13T18:40:25.185144Z",
     "shell.execute_reply": "2025-04-13T18:40:25.184439Z",
     "shell.execute_reply.started": "2025-04-13T18:40:13.351923Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upload successful!\n",
      "📁 File ID: 1fljYASJbnh0zWCQp9Ojy5igasTBNsbeZ\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/model_archive.zip\"  # Change this\n",
    "file_name = \"PYFIX_MODEL_2.zip\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "metadata = {\n",
    "    \"name\": file_name,\n",
    "    \"mimeType\": \"application/zip\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \"data\": (\"metadata\", json.dumps(metadata), \"application/json\"),\n",
    "    \"file\": open(file_path, \"rb\")\n",
    "}\n",
    "\n",
    "upload_url = \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\"\n",
    "\n",
    "res = requests.post(upload_url, headers=headers, files=files)\n",
    "res.raise_for_status()\n",
    "\n",
    "print(\"✅ Upload successful!\")\n",
    "print(\"📁 File ID:\", res.json()[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:41:04.571350Z",
     "iopub.status.busy": "2025-04-13T18:41:04.571064Z",
     "iopub.status.idle": "2025-04-13T18:41:04.741079Z",
     "shell.execute_reply": "2025-04-13T18:41:04.740298Z",
     "shell.execute_reply.started": "2025-04-13T18:41:04.571329Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_dir/tokenizer_config.json',\n",
       " 'tokenizer_dir/special_tokens_map.json',\n",
       " 'tokenizer_dir/vocab.json',\n",
       " 'tokenizer_dir/merges.txt',\n",
       " 'tokenizer_dir/added_tokens.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:41:15.514423Z",
     "iopub.status.busy": "2025-04-13T18:41:15.514131Z",
     "iopub.status.idle": "2025-04-13T18:41:15.623308Z",
     "shell.execute_reply": "2025-04-13T18:41:15.622738Z",
     "shell.execute_reply.started": "2025-04-13T18:41:15.514402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/tokenizer_dir.zip'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"tokenizer_dir\", 'zip', \"tokenizer_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer  # or your specific tokenizer\n",
    "\n",
    "# Load model\n",
    "model = torch.load(\"full_model.pth\", map_location=torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_dir\")\n",
    "\n",
    "# Example usage\n",
    "src_code = \"def add(x, y): return x + y\"\n",
    "tokens = tokenizer(src_code, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(tokens[\"input_ids\"], tokens[\"attention_mask\"], ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:31:39.186853Z",
     "iopub.status.busy": "2025-04-13T18:31:39.186261Z",
     "iopub.status.idle": "2025-04-13T18:32:04.450225Z",
     "shell.execute_reply": "2025-04-13T18:32:04.449449Z",
     "shell.execute_reply.started": "2025-04-13T18:31:39.186829Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_id': 'p02547_s853646132', 'src': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'z', ',', 'w', '=', '[', ']', ',', '[', ']', 'NEW_LINE', 'K', '=', '0', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'x', ',', 'y', '=', 'map', '(', 'int', ',', 'input', '(', ')', '.', 'split', '(', ')', ')', 'NEW_LINE', 'z', '.', 'append', '(', 'x', ')', 'NEW_LINE', 'w', '.', 'append', '(', 'y', ')', 'NEW_LINE', 'DEDENT', 'for', 'j', 'in', 'range', '(', '1', ',', 'N', '-', '1', ')', ':', 'NEW_LINE', 'INDENT', 'if', 'z', '[', 'j', ']', '==', 'w', '[', 'j', ']', 'and', 'z', '[', 'j', '+', '1', ']', '==', 'w', '[', 'j', '+', '1', ']', 'and', 'z', '[', 'j', '+', '2', ']', '==', 'w', '[', 'j', '+', '2', ']', ':', 'NEW_LINE', 'INDENT', 'K', '+=', '1', 'NEW_LINE', 'DEDENT', 'DEDENT', 'if', 'K', '>=', '1', ':', 'NEW_LINE', 'INDENT', 'print', '(', '\" Yes \"', ')', 'NEW_LINE', 'DEDENT', 'else', ':', 'NEW_LINE', 'INDENT', 'print', '(', '\" No \"', ')', 'NEW_LINE', 'DEDENT'], 'src_verdict': 'Runtime Error', 'tgt': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'z', ',', 'w', '=', '[', ']', ',', '[', ']', 'NEW_LINE', 'K', '=', '0', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'x', ',', 'y', '=', 'map', '(', 'int', ',', 'input', '(', ')', '.', 'split', '(', ')', ')', 'NEW_LINE', 'z', '.', 'append', '(', 'x', ')', 'NEW_LINE', 'w', '.', 'append', '(', 'y', ')', 'NEW_LINE', 'DEDENT', 'for', 'j', 'in', 'range', '(', 'N', '-', '2', ')', ':', 'NEW_LINE', 'INDENT', 'if', 'z', '[', 'j', ']', '==', 'w', '[', 'j', ']', 'and', 'z', '[', 'j', '+', '1', ']', '==', 'w', '[', 'j', '+', '1', ']', 'and', 'z', '[', 'j', '+', '2', ']', '==', 'w', '[', 'j', '+', '2', ']', ':', 'NEW_LINE', 'INDENT', 'K', '+=', '1', 'NEW_LINE', 'DEDENT', 'DEDENT', 'if', 'K', '>=', '1', ':', 'NEW_LINE', 'INDENT', 'print', '(', '\" Yes \"', ')', 'NEW_LINE', 'DEDENT', 'else', ':', 'NEW_LINE', 'INDENT', 'print', '(', '\" No \"', ')', 'NEW_LINE', 'DEDENT'], 'tgt_id': 'p02547_s000853418'}\n"
     ]
    }
   ],
   "source": [
    "test_file = \"/kaggle/input/code-net-test/test.jsonl\"\n",
    "test_data = load_and_preprocess_data(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:34:17.548441Z",
     "iopub.status.busy": "2025-04-13T18:34:17.547617Z",
     "iopub.status.idle": "2025-04-13T18:34:18.683669Z",
     "shell.execute_reply": "2025-04-13T18:34:18.683039Z",
     "shell.execute_reply.started": "2025-04-13T18:34:17.548410Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/2662461072.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('/kaggle/working/full_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Input (Buggy Code):\n",
      "N = int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( 1 , N - 1 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 1 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT\n",
      "\n",
      "✅ Prediction (Model Fix):\n",
      " = int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( 1 ) 1 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 2 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT  ( = = = = = = = = = =orioriorioriorioriorioriorioriorioriorioriorioriorioriorioriUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMakuakuakuUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMUMousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousandousand\n",
      "\n",
      "🎯 Target (Ground Truth Fix):\n",
      " N = int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( N - 2 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 1 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Load your saved model\n",
    "model = torch.load('/kaggle/working/full_model.pth')  \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# 2. Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "\n",
    "# 3. Prepare test data\n",
    "data_test = test_data[:1]  # or use more examples if needed\n",
    "\n",
    "# Tokenize the test data\n",
    "tokenized_test_data = pre_tokenize_data(data_test, tokenizer, max_length=512)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(tokenized_test_data, batch_size=1)  # batch_size=1 for clarity\n",
    "\n",
    "# 4. Run inference\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        src_input_ids = batch['src_input_ids'].to(device)\n",
    "        src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "        tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "        # 🔮 Predict (use teacher forcing)\n",
    "        output = model(src_input_ids, src_attention_mask, tgt_input_ids[:, :-1], tgt_attention_mask[:, :-1])\n",
    "        predicted_ids = output.argmax(dim=-1)\n",
    "\n",
    "        # Decode Input (buggy), Prediction, and Target (ground truth)\n",
    "        input_text = tokenizer.decode(src_input_ids[0], skip_special_tokens=True)\n",
    "        predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        target_text = tokenizer.decode(tgt_input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(\"🧪 Input (Buggy Code):\")\n",
    "        print(input_text)\n",
    "        print(\"\\n✅ Prediction (Model Fix):\")\n",
    "        print(predicted_text)\n",
    "        print(\"\\n🎯 Target (Ground Truth Fix):\")\n",
    "        print(target_text)\n",
    "        print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:46:35.816286Z",
     "iopub.status.busy": "2025-04-13T18:46:35.816009Z",
     "iopub.status.idle": "2025-04-13T18:46:35.820527Z",
     "shell.execute_reply": "2025-04-13T18:46:35.819845Z",
     "shell.execute_reply.started": "2025-04-13T18:46:35.816267Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_id': 'p02971_s801719045', 'src': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '=', '[', ']', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'a', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '.', 'append', '(', 'a', ')', 'NEW_LINE', 'DEDENT', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'if', 'A', '[', 'i', ']', '!=', 'max', '(', 'A', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'max', '(', 'A', ')', ')', 'NEW_LINE', 'DEDENT', 'else', ':', 'NEW_LINE', 'INDENT', 'p', '=', 'A', '[', 'i', ']', 'NEW_LINE', 'A', '[', 'i', ']', '=', '0', 'NEW_LINE', 'print', '(', 'max', '(', 'A', ')', ')', 'NEW_LINE', 'A', '[', 'i', ']', '=', 'p', 'NEW_LINE', 'DEDENT', 'DEDENT'], 'src_verdict': 'Time Limit Exceeded', 'tgt': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '=', '[', ']', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'a', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '.', 'append', '(', 'a', ')', 'NEW_LINE', 'DEDENT', 'AA', '=', 'sorted', '(', 'A', ',', 'reverse', '=', 'True', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'if', 'A', '[', 'i', ']', '!=', 'AA', '[', '0', ']', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'AA', '[', '0', ']', ')', 'NEW_LINE', 'DEDENT', 'else', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'AA', '[', '1', ']', ')', 'NEW_LINE', 'DEDENT', 'DEDENT'], 'tgt_id': 'p02971_s747975884'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[9])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6956758,
     "sourceId": 11150639,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7037387,
     "sourceId": 11259905,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7101948,
     "sourceId": 11350045,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7102129,
     "sourceId": 11350256,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
