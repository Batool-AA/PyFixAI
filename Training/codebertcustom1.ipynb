{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T11:28:46.210087Z",
     "iopub.status.busy": "2025-04-13T11:28:46.209351Z",
     "iopub.status.idle": "2025-04-13T11:29:23.874066Z",
     "shell.execute_reply": "2025-04-13T11:29:23.873307Z",
     "shell.execute_reply.started": "2025-04-13T11:28:46.210061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the CodeNet dataset for training.\"\"\"\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # Load all data at once\n",
    "\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        \n",
    "            preprocessed_data.append(entry)\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "# Replace with actual path\n",
    "train_file = \"/kaggle/input/code-net/train.jsonl\"\n",
    "train_data = load_and_preprocess_data(train_file)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:29:30.737693Z",
     "iopub.status.busy": "2025-04-13T11:29:30.737120Z",
     "iopub.status.idle": "2025-04-13T11:31:00.074976Z",
     "shell.execute_reply": "2025-04-13T11:31:00.074153Z",
     "shell.execute_reply.started": "2025-04-13T11:29:30.737653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:31:20.732625Z",
     "iopub.status.busy": "2025-04-13T11:31:20.732276Z",
     "iopub.status.idle": "2025-04-13T11:31:37.646502Z",
     "shell.execute_reply": "2025-04-13T11:31:37.645940Z",
     "shell.execute_reply.started": "2025-04-13T11:31:20.732600Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:31:42.275420Z",
     "iopub.status.busy": "2025-04-13T11:31:42.274555Z",
     "iopub.status.idle": "2025-04-13T11:32:01.301481Z",
     "shell.execute_reply": "2025-04-13T11:32:01.300919Z",
     "shell.execute_reply.started": "2025-04-13T11:31:42.275389Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "validation_file = \"/kaggle/input/code-net/valid.jsonl\"\n",
    "validation_data = load_and_preprocess_data(validation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_token_lengths(data, tokenizer):\n",
    "    src_lengths = []\n",
    "    tgt_lengths = []\n",
    "    for example in data:\n",
    "        src_text = \" \".join(example['src'])\n",
    "        tgt_text = \" \".join(example['tgt'])\n",
    "\n",
    "        src_enc = tokenizer(src_text, truncation=False)['input_ids']\n",
    "        tgt_enc = tokenizer(tgt_text, truncation=False)['input_ids']\n",
    "\n",
    "        src_lengths.append(len(src_enc))\n",
    "        tgt_lengths.append(len(tgt_enc))\n",
    "    \n",
    "    return src_lengths, tgt_lengths\n",
    "\n",
    "import numpy as np\n",
    "src_lengths,tgt_lengths = get_token_lengths(train_data[50000:80000],tokenizer)\n",
    "\n",
    "print(\"Source Lengths:\")\n",
    "print(f\"Mean: {np.mean(src_lengths):.2f}, 90th percentile: {np.percentile(src_lengths, 90)}, Max: {max(src_lengths)}\")\n",
    "\n",
    "print(\"\\nTarget Lengths:\")\n",
    "print(f\"Mean: {np.mean(tgt_lengths):.2f}, 90th percentile: {np.percentile(tgt_lengths, 90)}, Max: {max(tgt_lengths)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.model_max_length)  # This will print 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:32:08.719342Z",
     "iopub.status.busy": "2025-04-13T11:32:08.719031Z",
     "iopub.status.idle": "2025-04-13T11:35:03.797946Z",
     "shell.execute_reply": "2025-04-13T11:35:03.797300Z",
     "shell.execute_reply.started": "2025-04-13T11:32:08.719321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def encode_example(example, tokenizer, max_length=512):\n",
    "    src_tokens = example['src']\n",
    "    tgt_tokens = example['tgt']\n",
    "    src_text = \" \".join(src_tokens)\n",
    "    \n",
    "    # Add start and end tokens to the target\n",
    "    tgt_text = \"<s> \" + \" \".join(tgt_tokens) + \" </s>\"\n",
    "    \n",
    "    src_enc = tokenizer(src_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    tgt_enc = tokenizer(tgt_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return src_enc, tgt_enc\n",
    "\n",
    "\n",
    "\n",
    "class PreTokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_data[idx]\n",
    "\n",
    "def pre_tokenize_data(data, tokenizer, max_length=512):\n",
    "    tokenized_data = []\n",
    "    for example in data:\n",
    "        src_enc, tgt_enc = encode_example(example, tokenizer, max_length)\n",
    "        tokenized_data.append({\n",
    "            'src_input_ids': src_enc['input_ids'].squeeze(0),\n",
    "            'src_attention_mask': src_enc['attention_mask'].squeeze(0),\n",
    "            'tgt_input_ids': tgt_enc['input_ids'].squeeze(0),\n",
    "            'tgt_attention_mask': tgt_enc['attention_mask'].squeeze(0)\n",
    "        })\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_train_data = pre_tokenize_data(train_data[80000:130000], tokenizer, max_length=512)\n",
    "pretokenized_dataset = PreTokenizedDataset(tokenized_train_data)\n",
    "train_loader = DataLoader(pretokenized_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "tokenized_valid_data = pre_tokenize_data(validation_data[80000:130000], tokenizer, max_length=512)\n",
    "pretokenized_valid_dataset = PreTokenizedDataset(tokenized_valid_data)\n",
    "valid_loader = DataLoader(pretokenized_valid_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:35:16.846338Z",
     "iopub.status.busy": "2025-04-13T11:35:16.846059Z",
     "iopub.status.idle": "2025-04-13T11:35:16.856161Z",
     "shell.execute_reply": "2025-04-13T11:35:16.855385Z",
     "shell.execute_reply.started": "2025-04-13T11:35:16.846317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CodeErrorFixModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name, vocab_size, embed_size=768, num_decoder_layers=6, nhead=8):\n",
    "        super().__init__()\n",
    "        # Load the pretrained CodeBERT encoder\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model_name)\n",
    "        # Decoder components\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dropout=0.1)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Create a mask to ensure that each position only attends to previous positions\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask.to(next(self.parameters()).device)\n",
    "    \n",
    "    def forward(self, src_input_ids, src_attention_mask, tgt_input_ids, tgt_attention_mask):\n",
    "        # Encode source sequence\n",
    "        encoder_outputs = self.encoder(input_ids=src_input_ids, attention_mask=src_attention_mask)\n",
    "        memory = encoder_outputs.last_hidden_state  # shape: (batch_size, src_seq_len, embed_size)\n",
    "        \n",
    "        # Prepare target embeddings\n",
    "        tgt_embeddings = self.decoder_embedding(tgt_input_ids) * math.sqrt(self.embed_size)\n",
    "        tgt_embeddings = self.pos_encoder(tgt_embeddings)\n",
    "        # Transformer expects (seq_len, batch_size, embed_size)\n",
    "        tgt_embeddings = tgt_embeddings.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        tgt_seq_len = tgt_input_ids.size(1)\n",
    "        # Create target mask for auto-regressive generation\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        \n",
    "        decoder_output = self.decoder(tgt=tgt_embeddings, memory=memory, tgt_mask=tgt_mask)\n",
    "        # Transpose back: (batch_size, seq_len, embed_size)\n",
    "        decoder_output = decoder_output.transpose(0, 1)\n",
    "        logits = self.fc_out(decoder_output)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:35:22.828339Z",
     "iopub.status.busy": "2025-04-13T11:35:22.827858Z",
     "iopub.status.idle": "2025-04-13T11:36:00.953980Z",
     "shell.execute_reply": "2025-04-13T11:36:00.953148Z",
     "shell.execute_reply.started": "2025-04-13T11:35:22.828315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# === Step 1: Load tokenizer from saved folder ===\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/model-pyfix/tokenizer_dir\")\n",
    "\n",
    "# === Step 2: Load model from .pth ===\n",
    "model = torch.load(\"/kaggle/input/model-pyfix/PYFIX_MODEL/full_model.pth\")\n",
    "\n",
    "# === Step 3: Send to device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "# We'll ignore the padding tokens when computing loss\n",
    "pad_token_id = tokenizer.pad_token_id  # Make sure you have a tokenizer object\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:36:06.714278Z",
     "iopub.status.busy": "2025-04-13T11:36:06.713220Z",
     "iopub.status.idle": "2025-04-13T11:36:10.567532Z",
     "shell.execute_reply": "2025-04-13T11:36:10.566694Z",
     "shell.execute_reply.started": "2025-04-13T11:36:06.714232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:36:14.929892Z",
     "iopub.status.busy": "2025-04-13T11:36:14.929567Z",
     "iopub.status.idle": "2025-04-13T11:36:14.934811Z",
     "shell.execute_reply": "2025-04-13T11:36:14.934042Z",
     "shell.execute_reply.started": "2025-04-13T11:36:14.929865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T11:36:19.194458Z",
     "iopub.status.busy": "2025-04-13T11:36:19.194204Z",
     "iopub.status.idle": "2025-04-13T18:17:05.473339Z",
     "shell.execute_reply": "2025-04-13T18:17:05.472314Z",
     "shell.execute_reply.started": "2025-04-13T11:36:19.194442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "print(\"Training Started\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}\")):\n",
    "        src_input_ids = batch['src_input_ids'].to(device)\n",
    "        src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "        tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoder_input_ids = tgt_input_ids[:, :-1]\n",
    "        decoder_target_ids = tgt_input_ids[:, 1:]\n",
    "        \n",
    "        logits = model(\n",
    "            src_input_ids=src_input_ids,\n",
    "            src_attention_mask=src_attention_mask,\n",
    "            tgt_input_ids=decoder_input_ids,\n",
    "            tgt_attention_mask=tgt_attention_mask[:, :-1]\n",
    "        )\n",
    "        \n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        decoder_target_ids = decoder_target_ids.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits, decoder_target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Training Loss: {avg_loss:.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            src_input_ids = batch['src_input_ids'].to(device)\n",
    "            src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "            tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "            tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "            decoder_input_ids = tgt_input_ids[:, :-1]\n",
    "            decoder_target_ids = tgt_input_ids[:, 1:]\n",
    "\n",
    "            logits = model(\n",
    "                src_input_ids=src_input_ids,\n",
    "                src_attention_mask=src_attention_mask,\n",
    "                tgt_input_ids=decoder_input_ids,\n",
    "                tgt_attention_mask=tgt_attention_mask[:, :-1]\n",
    "            )\n",
    "\n",
    "            logits = logits.reshape(-1, vocab_size)\n",
    "            decoder_target_ids = decoder_target_ids.reshape(-1)\n",
    "\n",
    "            loss = criterion(logits, decoder_target_ids)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_avg_loss = val_loss / len(valid_loader)\n",
    "    print(f\" Epoch {epoch+1} - Validation Loss: {val_avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:31:04.480086Z",
     "iopub.status.busy": "2025-04-13T18:31:04.479558Z",
     "iopub.status.idle": "2025-04-13T18:31:06.138248Z",
     "shell.execute_reply": "2025-04-13T18:31:06.137612Z",
     "shell.execute_reply.started": "2025-04-13T18:31:04.480064Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"full_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:37:52.202124Z",
     "iopub.status.busy": "2025-04-13T18:37:52.201807Z",
     "iopub.status.idle": "2025-04-13T18:38:46.820325Z",
     "shell.execute_reply": "2025-04-13T18:38:46.819483Z",
     "shell.execute_reply.started": "2025-04-13T18:37:52.202091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r model_archive.zip full_model.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:39:14.435034Z",
     "iopub.status.busy": "2025-04-13T18:39:14.434350Z",
     "iopub.status.idle": "2025-04-13T18:40:09.872586Z",
     "shell.execute_reply": "2025-04-13T18:40:09.871895Z",
     "shell.execute_reply.started": "2025-04-13T18:39:14.435004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from google.colab import auth  # works in Kaggle too\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "creds.refresh(Request())\n",
    "access_token = creds.token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:40:13.351945Z",
     "iopub.status.busy": "2025-04-13T18:40:13.351197Z",
     "iopub.status.idle": "2025-04-13T18:40:25.185144Z",
     "shell.execute_reply": "2025-04-13T18:40:25.184439Z",
     "shell.execute_reply.started": "2025-04-13T18:40:13.351923Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/model_archive.zip\"  # Change this\n",
    "file_name = \"PYFIX_MODEL_2.zip\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "metadata = {\n",
    "    \"name\": file_name,\n",
    "    \"mimeType\": \"application/zip\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \"data\": (\"metadata\", json.dumps(metadata), \"application/json\"),\n",
    "    \"file\": open(file_path, \"rb\")\n",
    "}\n",
    "\n",
    "upload_url = \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\"\n",
    "\n",
    "res = requests.post(upload_url, headers=headers, files=files)\n",
    "res.raise_for_status()\n",
    "\n",
    "print(\"Upload successful!\")\n",
    "print(\"File ID:\", res.json()[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:41:04.571350Z",
     "iopub.status.busy": "2025-04-13T18:41:04.571064Z",
     "iopub.status.idle": "2025-04-13T18:41:04.741079Z",
     "shell.execute_reply": "2025-04-13T18:41:04.740298Z",
     "shell.execute_reply.started": "2025-04-13T18:41:04.571329Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:41:15.514423Z",
     "iopub.status.busy": "2025-04-13T18:41:15.514131Z",
     "iopub.status.idle": "2025-04-13T18:41:15.623308Z",
     "shell.execute_reply": "2025-04-13T18:41:15.622738Z",
     "shell.execute_reply.started": "2025-04-13T18:41:15.514402Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"tokenizer_dir\", 'zip', \"tokenizer_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer  # or your specific tokenizer\n",
    "\n",
    "# Load model\n",
    "model = torch.load(\"full_model.pth\", map_location=torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_dir\")\n",
    "\n",
    "# Example usage\n",
    "src_code = \"def add(x, y): return x + y\"\n",
    "tokens = tokenizer(src_code, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(tokens[\"input_ids\"], tokens[\"attention_mask\"], ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:31:39.186853Z",
     "iopub.status.busy": "2025-04-13T18:31:39.186261Z",
     "iopub.status.idle": "2025-04-13T18:32:04.450225Z",
     "shell.execute_reply": "2025-04-13T18:32:04.449449Z",
     "shell.execute_reply.started": "2025-04-13T18:31:39.186829Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_file = \"/kaggle/input/code-net-test/test.jsonl\"\n",
    "test_data = load_and_preprocess_data(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:34:17.548441Z",
     "iopub.status.busy": "2025-04-13T18:34:17.547617Z",
     "iopub.status.idle": "2025-04-13T18:34:18.683669Z",
     "shell.execute_reply": "2025-04-13T18:34:18.683039Z",
     "shell.execute_reply.started": "2025-04-13T18:34:17.548410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Load your saved model\n",
    "model = torch.load('/kaggle/working/full_model.pth')  \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# 2. Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "\n",
    "# 3. Prepare test data\n",
    "data_test = test_data[:1]  # or use more examples if needed\n",
    "\n",
    "# Tokenize the test data\n",
    "tokenized_test_data = pre_tokenize_data(data_test, tokenizer, max_length=512)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(tokenized_test_data, batch_size=1)  # batch_size=1 for clarity\n",
    "\n",
    "# 4. Run inference\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        src_input_ids = batch['src_input_ids'].to(device)\n",
    "        src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "        tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "      \n",
    "        output = model(src_input_ids, src_attention_mask, tgt_input_ids[:, :-1], tgt_attention_mask[:, :-1])\n",
    "        predicted_ids = output.argmax(dim=-1)\n",
    "\n",
    "        # Decode Input (buggy), Prediction, and Target (ground truth)\n",
    "        input_text = tokenizer.decode(src_input_ids[0], skip_special_tokens=True)\n",
    "        predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        target_text = tokenizer.decode(tgt_input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(\" Input (Buggy Code):\")\n",
    "        print(input_text)\n",
    "        print(\"\\nPrediction (Model Fix):\")\n",
    "        print(predicted_text)\n",
    "        print(\"\\nTarget (Ground Truth Fix):\")\n",
    "        print(target_text)\n",
    "        print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:46:35.816286Z",
     "iopub.status.busy": "2025-04-13T18:46:35.816009Z",
     "iopub.status.idle": "2025-04-13T18:46:35.820527Z",
     "shell.execute_reply": "2025-04-13T18:46:35.819845Z",
     "shell.execute_reply.started": "2025-04-13T18:46:35.816267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test_data[9])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6956758,
     "sourceId": 11150639,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7037387,
     "sourceId": 11259905,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7101948,
     "sourceId": 11350045,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7102129,
     "sourceId": 11350256,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
