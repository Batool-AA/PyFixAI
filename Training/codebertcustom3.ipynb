{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-15T08:05:38.930190Z",
     "iopub.status.busy": "2025-04-15T08:05:38.929457Z",
     "iopub.status.idle": "2025-04-15T08:06:16.706037Z",
     "shell.execute_reply": "2025-04-15T08:06:16.705363Z",
     "shell.execute_reply.started": "2025-04-15T08:05:38.930159Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the CodeNet dataset for training.\"\"\"\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # Load all data at once\n",
    "\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        \n",
    "            preprocessed_data.append(entry)\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "# Replace with actual path\n",
    "train_file = \"/kaggle/input/code-net-python/train.jsonl\"\n",
    "train_data = load_and_preprocess_data(train_file)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:06:20.864274Z",
     "iopub.status.busy": "2025-04-15T08:06:20.863652Z",
     "iopub.status.idle": "2025-04-15T08:07:35.618834Z",
     "shell.execute_reply": "2025-04-15T08:07:35.618107Z",
     "shell.execute_reply.started": "2025-04-15T08:06:20.864253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:08:08.392882Z",
     "iopub.status.busy": "2025-04-15T08:08:08.392573Z",
     "iopub.status.idle": "2025-04-15T08:08:21.421575Z",
     "shell.execute_reply": "2025-04-15T08:08:21.420999Z",
     "shell.execute_reply.started": "2025-04-15T08:08:08.392857Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:08:25.022968Z",
     "iopub.status.busy": "2025-04-15T08:08:25.022567Z",
     "iopub.status.idle": "2025-04-15T08:08:43.878297Z",
     "shell.execute_reply": "2025-04-15T08:08:43.877741Z",
     "shell.execute_reply.started": "2025-04-15T08:08:25.022946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "validation_file = \"/kaggle/input/code-net-python/valid.jsonl\"\n",
    "validation_data = load_and_preprocess_data(validation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_token_lengths(data, tokenizer):\n",
    "    src_lengths = []\n",
    "    tgt_lengths = []\n",
    "    for example in data:\n",
    "        src_text = \" \".join(example['src'])\n",
    "        tgt_text = \" \".join(example['tgt'])\n",
    "\n",
    "        src_enc = tokenizer(src_text, truncation=False)['input_ids']\n",
    "        tgt_enc = tokenizer(tgt_text, truncation=False)['input_ids']\n",
    "\n",
    "        src_lengths.append(len(src_enc))\n",
    "        tgt_lengths.append(len(tgt_enc))\n",
    "    \n",
    "    return src_lengths, tgt_lengths\n",
    "\n",
    "import numpy as np\n",
    "src_lengths,tgt_lengths = get_token_lengths(train_data[50000:80000],tokenizer)\n",
    "\n",
    "print(\"Source Lengths:\")\n",
    "print(f\"Mean: {np.mean(src_lengths):.2f}, 90th percentile: {np.percentile(src_lengths, 90)}, Max: {max(src_lengths)}\")\n",
    "\n",
    "print(\"\\nTarget Lengths:\")\n",
    "print(f\"Mean: {np.mean(tgt_lengths):.2f}, 90th percentile: {np.percentile(tgt_lengths, 90)}, Max: {max(tgt_lengths)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.model_max_length)  # This will print 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:08:51.645640Z",
     "iopub.status.busy": "2025-04-15T08:08:51.645329Z",
     "iopub.status.idle": "2025-04-15T08:13:49.360929Z",
     "shell.execute_reply": "2025-04-15T08:13:49.360313Z",
     "shell.execute_reply.started": "2025-04-15T08:08:51.645618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def encode_example(example, tokenizer, max_length=512):\n",
    "    src_tokens = example['src']\n",
    "    tgt_tokens = example['tgt']\n",
    "    src_text = \" \".join(src_tokens)\n",
    "    \n",
    "    # Add start and end tokens to the target\n",
    "    tgt_text = \"<s> \" + \" \".join(tgt_tokens) + \" </s>\"\n",
    "    \n",
    "    src_enc = tokenizer(src_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    tgt_enc = tokenizer(tgt_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return src_enc, tgt_enc\n",
    "\n",
    "\n",
    "\n",
    "class PreTokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_data[idx]\n",
    "\n",
    "def pre_tokenize_data(data, tokenizer, max_length=512):\n",
    "    tokenized_data = []\n",
    "    for example in data:\n",
    "        src_enc, tgt_enc = encode_example(example, tokenizer, max_length)\n",
    "        tokenized_data.append({\n",
    "            'src_input_ids': src_enc['input_ids'].squeeze(0),\n",
    "            'src_attention_mask': src_enc['attention_mask'].squeeze(0),\n",
    "            'tgt_input_ids': tgt_enc['input_ids'].squeeze(0),\n",
    "            'tgt_attention_mask': tgt_enc['attention_mask'].squeeze(0)\n",
    "        })\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_train_data = pre_tokenize_data(train_data[130000:180000], tokenizer, max_length=512)\n",
    "pretokenized_dataset = PreTokenizedDataset(tokenized_train_data)\n",
    "train_loader = DataLoader(pretokenized_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "tokenized_valid_data = pre_tokenize_data(validation_data[130000:180000], tokenizer, max_length=512)\n",
    "pretokenized_valid_dataset = PreTokenizedDataset(tokenized_valid_data)\n",
    "valid_loader = DataLoader(pretokenized_valid_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:15:17.198222Z",
     "iopub.status.busy": "2025-04-15T08:15:17.197742Z",
     "iopub.status.idle": "2025-04-15T08:15:17.208033Z",
     "shell.execute_reply": "2025-04-15T08:15:17.207313Z",
     "shell.execute_reply.started": "2025-04-15T08:15:17.198197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CodeErrorFixModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name, vocab_size, embed_size=768, num_decoder_layers=6, nhead=8):\n",
    "        super().__init__()\n",
    "        # Load the pretrained CodeBERT encoder\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model_name)\n",
    "        # Decoder components\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dropout=0.1)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Create a mask to ensure that each position only attends to previous positions\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask.to(next(self.parameters()).device)\n",
    "    \n",
    "    def forward(self, src_input_ids, src_attention_mask, tgt_input_ids, tgt_attention_mask):\n",
    "        # Encode source sequence\n",
    "        encoder_outputs = self.encoder(input_ids=src_input_ids, attention_mask=src_attention_mask)\n",
    "        memory = encoder_outputs.last_hidden_state  # shape: (batch_size, src_seq_len, embed_size)\n",
    "        \n",
    "        # Prepare target embeddings\n",
    "        tgt_embeddings = self.decoder_embedding(tgt_input_ids) * math.sqrt(self.embed_size)\n",
    "        tgt_embeddings = self.pos_encoder(tgt_embeddings)\n",
    "        # Transformer expects (seq_len, batch_size, embed_size)\n",
    "        tgt_embeddings = tgt_embeddings.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        tgt_seq_len = tgt_input_ids.size(1)\n",
    "        # Create target mask for auto-regressive generation\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        \n",
    "        decoder_output = self.decoder(tgt=tgt_embeddings, memory=memory, tgt_mask=tgt_mask)\n",
    "        # Transpose back: (batch_size, seq_len, embed_size)\n",
    "        decoder_output = decoder_output.transpose(0, 1)\n",
    "        logits = self.fc_out(decoder_output)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:15:20.868078Z",
     "iopub.status.busy": "2025-04-15T08:15:20.867801Z",
     "iopub.status.idle": "2025-04-15T08:15:50.423239Z",
     "shell.execute_reply": "2025-04-15T08:15:50.422516Z",
     "shell.execute_reply.started": "2025-04-15T08:15:20.868048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "# === Step 1: Load tokenizer from saved folder ===\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"/kaggle/input/model-3/no3\")\n",
    "\n",
    "# === Step 2: Load model from .pth ===\n",
    "model = torch.load(\"/kaggle/input/model-3/PYFIX_MODEL_3/full_model.pth\")\n",
    "\n",
    "# === Step 3: Send to device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "# We'll ignore the padding tokens when computing loss\n",
    "pad_token_id = tokenizer.pad_token_id  # Make sure you have a tokenizer object\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:15:57.580695Z",
     "iopub.status.busy": "2025-04-15T08:15:57.579677Z",
     "iopub.status.idle": "2025-04-15T08:16:01.132003Z",
     "shell.execute_reply": "2025-04-15T08:16:01.131258Z",
     "shell.execute_reply.started": "2025-04-15T08:15:57.580662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:16:03.828257Z",
     "iopub.status.busy": "2025-04-15T08:16:03.827960Z",
     "iopub.status.idle": "2025-04-15T08:16:03.832175Z",
     "shell.execute_reply": "2025-04-15T08:16:03.831427Z",
     "shell.execute_reply.started": "2025-04-15T08:16:03.828232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T08:16:07.952883Z",
     "iopub.status.busy": "2025-04-15T08:16:07.952531Z",
     "iopub.status.idle": "2025-04-15T16:25:51.480506Z",
     "shell.execute_reply": "2025-04-15T16:25:51.479776Z",
     "shell.execute_reply.started": "2025-04-15T08:16:07.952861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 2\n",
    "\n",
    "model.train()\n",
    "print(\"Training Started\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}\")):\n",
    "        src_input_ids = batch['src_input_ids'].to(device)\n",
    "        src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "        tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoder_input_ids = tgt_input_ids[:, :-1]\n",
    "        decoder_target_ids = tgt_input_ids[:, 1:]\n",
    "        \n",
    "        logits = model(\n",
    "            src_input_ids=src_input_ids,\n",
    "            src_attention_mask=src_attention_mask,\n",
    "            tgt_input_ids=decoder_input_ids,\n",
    "            tgt_attention_mask=tgt_attention_mask[:, :-1]\n",
    "        )\n",
    "        \n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        decoder_target_ids = decoder_target_ids.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits, decoder_target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Training Loss: {avg_loss:.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            src_input_ids = batch['src_input_ids'].to(device)\n",
    "            src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "            tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "            tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "            decoder_input_ids = tgt_input_ids[:, :-1]\n",
    "            decoder_target_ids = tgt_input_ids[:, 1:]\n",
    "\n",
    "            logits = model(\n",
    "                src_input_ids=src_input_ids,\n",
    "                src_attention_mask=src_attention_mask,\n",
    "                tgt_input_ids=decoder_input_ids,\n",
    "                tgt_attention_mask=tgt_attention_mask[:, :-1]\n",
    "            )\n",
    "\n",
    "            logits = logits.reshape(-1, vocab_size)\n",
    "            decoder_target_ids = decoder_target_ids.reshape(-1)\n",
    "\n",
    "            loss = criterion(logits, decoder_target_ids)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_avg_loss = val_loss / len(valid_loader)\n",
    "    print(f\" Epoch {epoch+1} - Validation Loss: {val_avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:40:17.076831Z",
     "iopub.status.busy": "2025-04-15T16:40:17.076488Z",
     "iopub.status.idle": "2025-04-15T16:40:18.810492Z",
     "shell.execute_reply": "2025-04-15T16:40:18.809741Z",
     "shell.execute_reply.started": "2025-04-15T16:40:17.076810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"full_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:40:44.284250Z",
     "iopub.status.busy": "2025-04-15T16:40:44.283560Z",
     "iopub.status.idle": "2025-04-15T16:41:36.316590Z",
     "shell.execute_reply": "2025-04-15T16:41:36.315867Z",
     "shell.execute_reply.started": "2025-04-15T16:40:44.284228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r model_archive.zip full_model.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:46:16.094327Z",
     "iopub.status.busy": "2025-04-15T16:46:16.093967Z",
     "iopub.status.idle": "2025-04-15T16:46:59.921992Z",
     "shell.execute_reply": "2025-04-15T16:46:59.921433Z",
     "shell.execute_reply.started": "2025-04-15T16:46:16.094300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from google.colab import auth  # works in Kaggle too\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "creds.refresh(Request())\n",
    "access_token = creds.token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:47:05.864092Z",
     "iopub.status.busy": "2025-04-15T16:47:05.863804Z",
     "iopub.status.idle": "2025-04-15T16:47:16.395424Z",
     "shell.execute_reply": "2025-04-15T16:47:16.394819Z",
     "shell.execute_reply.started": "2025-04-15T16:47:05.864073Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "file_path = \"/kaggle/working/model_archive.zip\"  # Change this\n",
    "file_name = \"PYFIX_MODEL_4.zip\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "metadata = {\n",
    "    \"name\": file_name,\n",
    "    \"mimeType\": \"application/zip\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \"data\": (\"metadata\", json.dumps(metadata), \"application/json\"),\n",
    "    \"file\": open(file_path, \"rb\")\n",
    "}\n",
    "\n",
    "upload_url = \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\"\n",
    "\n",
    "res = requests.post(upload_url, headers=headers, files=files)\n",
    "res.raise_for_status()\n",
    "\n",
    "print(\"Upload successful!\")\n",
    "print(\"File ID:\", res.json()[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:47:41.652958Z",
     "iopub.status.busy": "2025-04-15T16:47:41.652361Z",
     "iopub.status.idle": "2025-04-15T16:47:41.742160Z",
     "shell.execute_reply": "2025-04-15T16:47:41.741357Z",
     "shell.execute_reply.started": "2025-04-15T16:47:41.652936Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:47:44.935274Z",
     "iopub.status.busy": "2025-04-15T16:47:44.934677Z",
     "iopub.status.idle": "2025-04-15T16:47:45.040159Z",
     "shell.execute_reply": "2025-04-15T16:47:45.039561Z",
     "shell.execute_reply.started": "2025-04-15T16:47:44.935252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"tokenizer_dir\", 'zip', \"tokenizer_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer  # or your specific tokenizer\n",
    "\n",
    "# Load model\n",
    "model = torch.load(\"full_model.pth\", map_location=torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_dir\")\n",
    "\n",
    "# Example usage\n",
    "src_code = \"def add(x, y): return x + y\"\n",
    "tokens = tokenizer(src_code, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(tokens[\"input_ids\"], tokens[\"attention_mask\"], ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:49:48.303693Z",
     "iopub.status.busy": "2025-04-15T16:49:48.303396Z",
     "iopub.status.idle": "2025-04-15T16:50:13.199043Z",
     "shell.execute_reply": "2025-04-15T16:50:13.198431Z",
     "shell.execute_reply.started": "2025-04-15T16:49:48.303673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_file = \"/kaggle/input/code-net-python/test.jsonl\"\n",
    "test_data = load_and_preprocess_data(test_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:56:20.805174Z",
     "iopub.status.busy": "2025-04-15T16:56:20.804872Z",
     "iopub.status.idle": "2025-04-15T16:56:20.809671Z",
     "shell.execute_reply": "2025-04-15T16:56:20.808910Z",
     "shell.execute_reply.started": "2025-04-15T16:56:20.805153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_prediction(text):\n",
    "    # Remove common garbage patterns\n",
    "    text = re.sub(r\"(=|NEW|\\\\n|\\+|\\s){5,}\", \"\", text)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s\\.\\,\\:\\(\\)\\[\\]\\+\\-\\*/\\<\\>=\\'\\\"\\_]+\", \"\", text)  # remove weird tokens\n",
    "    text = text.strip()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T16:56:50.555057Z",
     "iopub.status.busy": "2025-04-15T16:56:50.554359Z",
     "iopub.status.idle": "2025-04-15T16:56:51.722000Z",
     "shell.execute_reply": "2025-04-15T16:56:51.721211Z",
     "shell.execute_reply.started": "2025-04-15T16:56:50.555033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Load your saved model\n",
    "model = torch.load('/kaggle/input/model-pyfix/PYFIX_MODEL/full_model.pth')  \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# 2. Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "\n",
    "# 3. Prepare test data\n",
    "data_test = test_data[:1]  # or use more examples if needed\n",
    "\n",
    "# Tokenize the test data\n",
    "tokenized_test_data = pre_tokenize_data(data_test, tokenizer, max_length=512)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(tokenized_test_data, batch_size=1)  # batch_size=1 for clarity\n",
    "\n",
    "# 4. Run inference\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        src_input_ids = batch['src_input_ids'].to(device)\n",
    "        src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "        tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "        # 🔮 Predict (use teacher forcing)\n",
    "        output = model(src_input_ids, src_attention_mask, tgt_input_ids[:, :-1], tgt_attention_mask[:, :-1])\n",
    "        predicted_ids = output.argmax(dim=-1)\n",
    "\n",
    "        # Decode Input (buggy), Prediction, and Target (ground truth)\n",
    "        input_text = tokenizer.decode(src_input_ids[0], skip_special_tokens=True)\n",
    "        predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        target_text = tokenizer.decode(tgt_input_ids[0], skip_special_tokens=True)\n",
    "        predicted_text = clean_prediction(predicted_text)\n",
    "\n",
    "        print(\" Input (Buggy Code):\")\n",
    "        print(input_text)\n",
    "        print(\"\\n Prediction (Model Fix):\")\n",
    "        print(predicted_text)\n",
    "        print(\"\\n Target (Ground Truth Fix):\")\n",
    "        print(target_text)\n",
    "        print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T18:46:35.816286Z",
     "iopub.status.busy": "2025-04-13T18:46:35.816009Z",
     "iopub.status.idle": "2025-04-13T18:46:35.820527Z",
     "shell.execute_reply": "2025-04-13T18:46:35.819845Z",
     "shell.execute_reply.started": "2025-04-13T18:46:35.816267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(test_data[9])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6956758,
     "sourceId": 11150639,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7037387,
     "sourceId": 11259905,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7101948,
     "sourceId": 11350045,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7102129,
     "sourceId": 11350256,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7138777,
     "sourceId": 11398360,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7149610,
     "sourceId": 11415698,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
