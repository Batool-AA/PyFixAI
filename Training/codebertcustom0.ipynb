{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-08T06:14:15.205036Z",
     "iopub.status.busy": "2025-04-08T06:14:15.204714Z",
     "iopub.status.idle": "2025-04-08T06:14:53.249866Z",
     "shell.execute_reply": "2025-04-08T06:14:53.249035Z",
     "shell.execute_reply.started": "2025-04-08T06:14:15.205011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_id': 'p00001_s631177546', 'src': ['from', 'sys', 'import', 'stdin', 'NEW_LINE', 'x', '=', '[', 'int', '(', 'input', '(', ')', ')', 'for', 'i', 'in', 'range', '(', '10', ')', ']', 'NEW_LINE', 'x', '.', 'reverse', '(', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '3', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'i', ')', 'NEW_LINE', 'DEDENT'], 'src_verdict': 'Wrong Answer', 'tgt': ['from', 'sys', 'import', 'stdin', 'NEW_LINE', 'x', '=', '[', 'int', '(', 'input', '(', ')', ')', 'for', 'i', 'in', 'range', '(', '10', ')', ']', 'NEW_LINE', 'x', '.', 'sort', '(', 'reverse', '=', 'True', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '3', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'x', '[', 'i', ']', ')', 'NEW_LINE', 'DEDENT'], 'tgt_id': 'p00001_s854661751'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads and preprocesses the CodeNet dataset for training.\"\"\"\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # Load all data at once\n",
    "\n",
    "    preprocessed_data = []\n",
    "\n",
    "    for i, entry in enumerate(data):\n",
    "        \n",
    "            preprocessed_data.append(entry)\n",
    "\n",
    "    return preprocessed_data\n",
    "\n",
    "# Replace with actual path\n",
    "train_file = \"/kaggle/input/code-net-python/train.jsonl\"\n",
    "train_data = load_and_preprocess_data(train_file)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:15:34.343670Z",
     "iopub.status.busy": "2025-04-08T06:15:34.343354Z",
     "iopub.status.idle": "2025-04-08T06:15:38.889287Z",
     "shell.execute_reply": "2025-04-08T06:15:38.888379Z",
     "shell.execute_reply.started": "2025-04-08T06:15:34.343643Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:24:19.145781Z",
     "iopub.status.busy": "2025-04-08T06:24:19.145427Z",
     "iopub.status.idle": "2025-04-08T06:24:31.906539Z",
     "shell.execute_reply": "2025-04-08T06:24:31.905839Z",
     "shell.execute_reply.started": "2025-04-08T06:24:19.145755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67647ce2b7a949d7bf09426dc065882d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fb1a0283844033bbd51c3e884b2b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759075192b804fffb01df444b55833e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb041aa6c2d4c8d9588eed3750e83f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa759b51f8b84978880917e4446b02b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:27:29.122748Z",
     "iopub.status.busy": "2025-04-08T06:27:29.122251Z",
     "iopub.status.idle": "2025-04-08T06:28:00.740653Z",
     "shell.execute_reply": "2025-04-08T06:28:00.739711Z",
     "shell.execute_reply.started": "2025-04-08T06:27:29.122722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "validation_file = \"/kaggle/input/code-net-python/valid.jsonl\"\n",
    "validation_data = load_and_preprocess_data(validation_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_token_lengths(data, tokenizer):\n",
    "    src_lengths = []\n",
    "    tgt_lengths = []\n",
    "    for example in data:\n",
    "        src_text = \" \".join(example['src'])\n",
    "        tgt_text = \" \".join(example['tgt'])\n",
    "\n",
    "        src_enc = tokenizer(src_text, truncation=False)['input_ids']\n",
    "        tgt_enc = tokenizer(tgt_text, truncation=False)['input_ids']\n",
    "\n",
    "        src_lengths.append(len(src_enc))\n",
    "        tgt_lengths.append(len(tgt_enc))\n",
    "    \n",
    "    return src_lengths, tgt_lengths\n",
    "\n",
    "import numpy as np\n",
    "src_lengths,tgt_lengths = get_token_lengths(train_data[:50000],tokenizer)\n",
    "\n",
    "print(\"Source Lengths:\")\n",
    "print(f\"Mean: {np.mean(src_lengths):.2f}, 90th percentile: {np.percentile(src_lengths, 90)}, Max: {max(src_lengths)}\")\n",
    "\n",
    "print(\"\\nTarget Lengths:\")\n",
    "print(f\"Mean: {np.mean(tgt_lengths):.2f}, 90th percentile: {np.percentile(tgt_lengths, 90)}, Max: {max(tgt_lengths)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(tokenizer.model_max_length)  # This will print 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:28:19.629562Z",
     "iopub.status.busy": "2025-04-08T06:28:19.629258Z",
     "iopub.status.idle": "2025-04-08T06:33:15.933328Z",
     "shell.execute_reply": "2025-04-08T06:33:15.932418Z",
     "shell.execute_reply.started": "2025-04-08T06:28:19.629541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "def encode_example(example, tokenizer, max_length=512):\n",
    "    src_tokens = example['src']\n",
    "    tgt_tokens = example['tgt']\n",
    "    src_text = \" \".join(src_tokens)\n",
    "    \n",
    "    # Add start and end tokens to the target\n",
    "    tgt_text = \"<s> \" + \" \".join(tgt_tokens) + \" </s>\"\n",
    "    \n",
    "    src_enc = tokenizer(src_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    tgt_enc = tokenizer(tgt_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return src_enc, tgt_enc\n",
    "\n",
    "\n",
    "\n",
    "class PreTokenizedDataset(Dataset):\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.tokenized_data = tokenized_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tokenized_data[idx]\n",
    "\n",
    "def pre_tokenize_data(data, tokenizer, max_length=512):\n",
    "    tokenized_data = []\n",
    "    for example in data:\n",
    "        src_enc, tgt_enc = encode_example(example, tokenizer, max_length)\n",
    "        tokenized_data.append({\n",
    "            'src_input_ids': src_enc['input_ids'].squeeze(0),\n",
    "            'src_attention_mask': src_enc['attention_mask'].squeeze(0),\n",
    "            'tgt_input_ids': tgt_enc['input_ids'].squeeze(0),\n",
    "            'tgt_attention_mask': tgt_enc['attention_mask'].squeeze(0)\n",
    "        })\n",
    "    return tokenized_data\n",
    "\n",
    "tokenized_train_data = pre_tokenize_data(train_data[:50000], tokenizer, max_length=512)\n",
    "pretokenized_dataset = PreTokenizedDataset(tokenized_train_data)\n",
    "train_loader = DataLoader(pretokenized_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "tokenized_valid_data = pre_tokenize_data(validation_data[:50000], tokenizer, max_length=512)\n",
    "pretokenized_valid_dataset = PreTokenizedDataset(tokenized_valid_data)\n",
    "valid_loader = DataLoader(pretokenized_valid_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:33:33.194040Z",
     "iopub.status.busy": "2025-04-08T06:33:33.193653Z",
     "iopub.status.idle": "2025-04-08T06:33:33.203999Z",
     "shell.execute_reply": "2025-04-08T06:33:33.203049Z",
     "shell.execute_reply.started": "2025-04-08T06:33:33.194010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CodeErrorFixModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name, vocab_size, embed_size=768, num_decoder_layers=6, nhead=8):\n",
    "        super().__init__()\n",
    "        # Load the pretrained CodeBERT encoder\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model_name)\n",
    "        # Decoder components\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dropout=0.1)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Create a mask to ensure that each position only attends to previous positions\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask.to(next(self.parameters()).device)\n",
    "    \n",
    "    def forward(self, src_input_ids, src_attention_mask, tgt_input_ids, tgt_attention_mask):\n",
    "        # Encode source sequence\n",
    "        encoder_outputs = self.encoder(input_ids=src_input_ids, attention_mask=src_attention_mask)\n",
    "        memory = encoder_outputs.last_hidden_state  # shape: (batch_size, src_seq_len, embed_size)\n",
    "        \n",
    "        # Prepare target embeddings\n",
    "        tgt_embeddings = self.decoder_embedding(tgt_input_ids) * math.sqrt(self.embed_size)\n",
    "        tgt_embeddings = self.pos_encoder(tgt_embeddings)\n",
    "        # Transformer expects (seq_len, batch_size, embed_size)\n",
    "        tgt_embeddings = tgt_embeddings.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        tgt_seq_len = tgt_input_ids.size(1)\n",
    "        # Create target mask for auto-regressive generation\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        \n",
    "        decoder_output = self.decoder(tgt=tgt_embeddings, memory=memory, tgt_mask=tgt_mask)\n",
    "        # Transpose back: (batch_size, seq_len, embed_size)\n",
    "        decoder_output = decoder_output.transpose(0, 1)\n",
    "        logits = self.fc_out(decoder_output)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:33:39.788070Z",
     "iopub.status.busy": "2025-04-08T06:33:39.787705Z",
     "iopub.status.idle": "2025-04-08T06:34:03.571763Z",
     "shell.execute_reply": "2025-04-08T06:34:03.570879Z",
     "shell.execute_reply.started": "2025-04-08T06:33:39.788041Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb26656598d453f91ff72695c8124d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = CodeErrorFixModel(encoder_model_name=\"microsoft/codebert-base\", vocab_size=vocab_size)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "# We'll ignore the padding tokens when computing loss\n",
    "pad_token_id = tokenizer.pad_token_id  # Make sure you have a tokenizer object\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:34:10.169934Z",
     "iopub.status.busy": "2025-04-08T06:34:10.168973Z",
     "iopub.status.idle": "2025-04-08T06:34:14.059790Z",
     "shell.execute_reply": "2025-04-08T06:34:14.058689Z",
     "shell.execute_reply.started": "2025-04-08T06:34:10.169901Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T06:34:32.439269Z",
     "iopub.status.busy": "2025-04-08T06:34:32.438888Z",
     "iopub.status.idle": "2025-04-08T17:38:43.253925Z",
     "shell.execute_reply": "2025-04-08T17:38:43.252792Z",
     "shell.execute_reply.started": "2025-04-08T06:34:32.439239Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0215b03b8caa40f8b90cb94f6c75bd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/3 - Average Training Loss: 1.2159\n",
      "🧪 Epoch 1 - Validation Loss: 0.5936\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5d75687e56491d8bf38001e96b4929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2/3 - Average Training Loss: 0.4979\n",
      "🧪 Epoch 2 - Validation Loss: 0.3705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c087713747d5425b8f3df10aceb4df37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3/3 - Average Training Loss: 0.3346\n",
      "🧪 Epoch 3 - Validation Loss: 0.2793\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "num_epochs = 3\n",
    "\n",
    "model.train()\n",
    "print(\"Training Started\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader, total=len(train_loader), desc=f\"Epoch {epoch+1}\")):\n",
    "        src_input_ids = batch['src_input_ids'].to(device)\n",
    "        src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "        tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        decoder_input_ids = tgt_input_ids[:, :-1]\n",
    "        decoder_target_ids = tgt_input_ids[:, 1:]\n",
    "        \n",
    "        logits = model(\n",
    "            src_input_ids=src_input_ids,\n",
    "            src_attention_mask=src_attention_mask,\n",
    "            tgt_input_ids=decoder_input_ids,\n",
    "            tgt_attention_mask=tgt_attention_mask[:, :-1]\n",
    "        )\n",
    "        \n",
    "        logits = logits.reshape(-1, vocab_size)\n",
    "        decoder_target_ids = decoder_target_ids.reshape(-1)\n",
    "        \n",
    "        loss = criterion(logits, decoder_target_ids)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"✅ Epoch {epoch+1}/{num_epochs} - Average Training Loss: {avg_loss:.4f}\")\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            src_input_ids = batch['src_input_ids'].to(device)\n",
    "            src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "            tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "            tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "            decoder_input_ids = tgt_input_ids[:, :-1]\n",
    "            decoder_target_ids = tgt_input_ids[:, 1:]\n",
    "\n",
    "            logits = model(\n",
    "                src_input_ids=src_input_ids,\n",
    "                src_attention_mask=src_attention_mask,\n",
    "                tgt_input_ids=decoder_input_ids,\n",
    "                tgt_attention_mask=tgt_attention_mask[:, :-1]\n",
    "            )\n",
    "\n",
    "            logits = logits.reshape(-1, vocab_size)\n",
    "            decoder_target_ids = decoder_target_ids.reshape(-1)\n",
    "\n",
    "            loss = criterion(logits, decoder_target_ids)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_avg_loss = val_loss / len(valid_loader)\n",
    "    print(f\"🧪 Epoch {epoch+1} - Validation Loss: {val_avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:42:08.355505Z",
     "iopub.status.busy": "2025-04-08T17:42:08.355162Z",
     "iopub.status.idle": "2025-04-08T17:42:09.948522Z",
     "shell.execute_reply": "2025-04-08T17:42:09.947876Z",
     "shell.execute_reply.started": "2025-04-08T17:42:08.355479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.save(model, \"full_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:42:20.514168Z",
     "iopub.status.busy": "2025-04-08T17:42:20.513686Z",
     "iopub.status.idle": "2025-04-08T17:43:11.892736Z",
     "shell.execute_reply": "2025-04-08T17:43:11.891865Z",
     "shell.execute_reply.started": "2025-04-08T17:42:20.514126Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: full_model.pth (deflated 7%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r model_archive.zip full_model.pth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:43:27.487456Z",
     "iopub.status.busy": "2025-04-08T17:43:27.487073Z",
     "iopub.status.idle": "2025-04-08T17:44:06.306598Z",
     "shell.execute_reply": "2025-04-08T17:44:06.305987Z",
     "shell.execute_reply.started": "2025-04-08T17:43:27.487417Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from google.colab import auth  # works in Kaggle too\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "auth.authenticate_user()\n",
    "creds, _ = google.auth.default()\n",
    "creds.refresh(Request())\n",
    "access_token = creds.token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:44:11.169798Z",
     "iopub.status.busy": "2025-04-08T17:44:11.169489Z",
     "iopub.status.idle": "2025-04-08T17:44:23.857529Z",
     "shell.execute_reply": "2025-04-08T17:44:23.856770Z",
     "shell.execute_reply.started": "2025-04-08T17:44:11.169773Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upload successful!\n",
      "📁 File ID: 1C2L_ibFfqMzCGTzIOGllocU13OdAljcc\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/kaggle/working/model_archive.zip\"  # Change this\n",
    "file_name = \"PYFIX_MODEL.zip\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "metadata = {\n",
    "    \"name\": file_name,\n",
    "    \"mimeType\": \"application/zip\"\n",
    "}\n",
    "\n",
    "files = {\n",
    "    \"data\": (\"metadata\", json.dumps(metadata), \"application/json\"),\n",
    "    \"file\": open(file_path, \"rb\")\n",
    "}\n",
    "\n",
    "upload_url = \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\"\n",
    "\n",
    "res = requests.post(upload_url, headers=headers, files=files)\n",
    "res.raise_for_status()\n",
    "\n",
    "print(\"✅ Upload successful!\")\n",
    "print(\"📁 File ID:\", res.json()[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:44:35.478641Z",
     "iopub.status.busy": "2025-04-08T17:44:35.478360Z",
     "iopub.status.idle": "2025-04-08T17:44:35.617431Z",
     "shell.execute_reply": "2025-04-08T17:44:35.616730Z",
     "shell.execute_reply.started": "2025-04-08T17:44:35.478621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_dir/tokenizer_config.json',\n",
       " 'tokenizer_dir/special_tokens_map.json',\n",
       " 'tokenizer_dir/vocab.json',\n",
       " 'tokenizer_dir/merges.txt',\n",
       " 'tokenizer_dir/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"tokenizer_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:44:59.108957Z",
     "iopub.status.busy": "2025-04-08T17:44:59.108624Z",
     "iopub.status.idle": "2025-04-08T17:44:59.211017Z",
     "shell.execute_reply": "2025-04-08T17:44:59.210346Z",
     "shell.execute_reply.started": "2025-04-08T17:44:59.108933Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/tokenizer_dir.zip'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"tokenizer_dir\", 'zip', \"tokenizer_dir\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer  # or your specific tokenizer\n",
    "\n",
    "# Load model\n",
    "model = torch.load(\"full_model.pth\", map_location=torch.device(\"cpu\"))\n",
    "model.eval()\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_dir\")\n",
    "\n",
    "# Example usage\n",
    "src_code = \"def add(x, y): return x + y\"\n",
    "tokens = tokenizer(src_code, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    output = model(tokens[\"input_ids\"], tokens[\"attention_mask\"], ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:45:19.571760Z",
     "iopub.status.busy": "2025-04-08T17:45:19.571474Z",
     "iopub.status.idle": "2025-04-08T17:45:34.967838Z",
     "shell.execute_reply": "2025-04-08T17:45:34.967100Z",
     "shell.execute_reply.started": "2025-04-08T17:45:19.571738Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_id': 'p02547_s853646132', 'src': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'z', ',', 'w', '=', '[', ']', ',', '[', ']', 'NEW_LINE', 'K', '=', '0', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'x', ',', 'y', '=', 'map', '(', 'int', ',', 'input', '(', ')', '.', 'split', '(', ')', ')', 'NEW_LINE', 'z', '.', 'append', '(', 'x', ')', 'NEW_LINE', 'w', '.', 'append', '(', 'y', ')', 'NEW_LINE', 'DEDENT', 'for', 'j', 'in', 'range', '(', '1', ',', 'N', '-', '1', ')', ':', 'NEW_LINE', 'INDENT', 'if', 'z', '[', 'j', ']', '==', 'w', '[', 'j', ']', 'and', 'z', '[', 'j', '+', '1', ']', '==', 'w', '[', 'j', '+', '1', ']', 'and', 'z', '[', 'j', '+', '2', ']', '==', 'w', '[', 'j', '+', '2', ']', ':', 'NEW_LINE', 'INDENT', 'K', '+=', '1', 'NEW_LINE', 'DEDENT', 'DEDENT', 'if', 'K', '>=', '1', ':', 'NEW_LINE', 'INDENT', 'print', '(', '\" Yes \"', ')', 'NEW_LINE', 'DEDENT', 'else', ':', 'NEW_LINE', 'INDENT', 'print', '(', '\" No \"', ')', 'NEW_LINE', 'DEDENT'], 'src_verdict': 'Runtime Error', 'tgt': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'z', ',', 'w', '=', '[', ']', ',', '[', ']', 'NEW_LINE', 'K', '=', '0', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'x', ',', 'y', '=', 'map', '(', 'int', ',', 'input', '(', ')', '.', 'split', '(', ')', ')', 'NEW_LINE', 'z', '.', 'append', '(', 'x', ')', 'NEW_LINE', 'w', '.', 'append', '(', 'y', ')', 'NEW_LINE', 'DEDENT', 'for', 'j', 'in', 'range', '(', 'N', '-', '2', ')', ':', 'NEW_LINE', 'INDENT', 'if', 'z', '[', 'j', ']', '==', 'w', '[', 'j', ']', 'and', 'z', '[', 'j', '+', '1', ']', '==', 'w', '[', 'j', '+', '1', ']', 'and', 'z', '[', 'j', '+', '2', ']', '==', 'w', '[', 'j', '+', '2', ']', ':', 'NEW_LINE', 'INDENT', 'K', '+=', '1', 'NEW_LINE', 'DEDENT', 'DEDENT', 'if', 'K', '>=', '1', ':', 'NEW_LINE', 'INDENT', 'print', '(', '\" Yes \"', ')', 'NEW_LINE', 'DEDENT', 'else', ':', 'NEW_LINE', 'INDENT', 'print', '(', '\" No \"', ')', 'NEW_LINE', 'DEDENT'], 'tgt_id': 'p02547_s000853418'}\n"
     ]
    }
   ],
   "source": [
    "test_file = \"/kaggle/input/code-net-python/test.jsonl\"\n",
    "test_data = load_and_preprocess_data(test_file)\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-08T17:50:03.426105Z",
     "iopub.status.busy": "2025-04-08T17:50:03.425711Z",
     "iopub.status.idle": "2025-04-08T17:50:04.601487Z",
     "shell.execute_reply": "2025-04-08T17:50:04.600718Z",
     "shell.execute_reply.started": "2025-04-08T17:50:03.426076Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-c7f885f0592f>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load('/kaggle/working/full_model.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Input (Buggy Code):\n",
      "N = int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( 1 , N - 1 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 1 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT\n",
      "\n",
      "✅ Prediction (Model Fix):\n",
      " = int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( 1 ) 1 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 2 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT  = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = NEW NEW NEW NEW NEW = = = NEW NEW NEW NEW = = = = = = = = = = = = = = = = = = = = = = = = = = = = = NEW NEW NEW = = = = NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW NEW = = = = + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +\n",
      "\n",
      "🎯 Target (Ground Truth Fix):\n",
      " N = int ( input ( ) ) NEW_LINE z , w = [ ] , [ ] NEW_LINE K = 0 NEW_LINE for i in range ( N ) : NEW_LINE INDENT x , y = map ( int , input ( ) . split ( ) ) NEW_LINE z . append ( x ) NEW_LINE w . append ( y ) NEW_LINE DEDENT for j in range ( N - 2 ) : NEW_LINE INDENT if z [ j ] == w [ j ] and z [ j + 1 ] == w [ j + 1 ] and z [ j + 2 ] == w [ j + 2 ] : NEW_LINE INDENT K += 1 NEW_LINE DEDENT DEDENT if K >= 1 : NEW_LINE INDENT print ( \" Yes \" ) NEW_LINE DEDENT else : NEW_LINE INDENT print ( \" No \" ) NEW_LINE DEDENT \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Load your saved model\n",
    "model = torch.load('/kaggle/working/full_model.pth')  \n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# 2. Load the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
    "\n",
    "# 3. Prepare test data\n",
    "data_test = test_data[:1]  # or use more examples if needed\n",
    "\n",
    "# Tokenize the test data\n",
    "tokenized_test_data = pre_tokenize_data(data_test, tokenizer, max_length=512)\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(tokenized_test_data, batch_size=1)  # batch_size=1 for clarity\n",
    "\n",
    "# 4. Run inference\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        src_input_ids = batch['src_input_ids'].to(device)\n",
    "        src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "        tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "        tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "        # 🔮 Predict (use teacher forcing)\n",
    "        output = model(src_input_ids, src_attention_mask, tgt_input_ids[:, :-1], tgt_attention_mask[:, :-1])\n",
    "        predicted_ids = output.argmax(dim=-1)\n",
    "\n",
    "        # Decode Input (buggy), Prediction, and Target (ground truth)\n",
    "        input_text = tokenizer.decode(src_input_ids[0], skip_special_tokens=True)\n",
    "        predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "        target_text = tokenizer.decode(tgt_input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        print(\"🧪 Input (Buggy Code):\")\n",
    "        print(input_text)\n",
    "        print(\"\\n✅ Prediction (Model Fix):\")\n",
    "        print(predicted_text)\n",
    "        print(\"\\n🎯 Target (Ground Truth Fix):\")\n",
    "        print(target_text)\n",
    "        print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7037387,
     "sourceId": 11259905,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
