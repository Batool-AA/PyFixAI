{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11146291,"sourceType":"datasetVersion","datasetId":6953452},{"sourceId":11298725,"sourceType":"datasetVersion","datasetId":7065527}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaTokenizer, GPT2Tokenizer, EncoderDecoderModel, AdamW","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:52:37.727424Z","iopub.execute_input":"2025-04-06T14:52:37.727728Z","iopub.status.idle":"2025-04-06T14:52:57.802709Z","shell.execute_reply.started":"2025-04-06T14:52:37.727706Z","shell.execute_reply":"2025-04-06T14:52:57.801810Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import json\n\ndef load_and_preprocess_data(file_path, limit=20000):\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return data[:limit]\n\n# Replace with actual path\ntrain_file = \"/kaggle/input/pyfixai/train.jsonl\"\ntrain_data = load_and_preprocess_data(train_file)\n\nprint(f\"Training Samples: {len(train_data)}\")\nprint(train_data[0])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:30:59.266162Z","iopub.execute_input":"2025-04-06T07:30:59.266445Z","iopub.status.idle":"2025-04-06T07:31:41.192093Z","shell.execute_reply.started":"2025-04-06T07:30:59.266423Z","shell.execute_reply":"2025-04-06T07:31:41.191122Z"}},"outputs":[{"name":"stdout","text":"Training Samples: 20000\n{'src_id': 'p00001_s631177546', 'src': ['from', 'sys', 'import', 'stdin', 'NEW_LINE', 'x', '=', '[', 'int', '(', 'input', '(', ')', ')', 'for', 'i', 'in', 'range', '(', '10', ')', ']', 'NEW_LINE', 'x', '.', 'reverse', '(', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '3', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'i', ')', 'NEW_LINE', 'DEDENT'], 'src_verdict': 'Wrong Answer', 'tgt': ['from', 'sys', 'import', 'stdin', 'NEW_LINE', 'x', '=', '[', 'int', '(', 'input', '(', ')', ')', 'for', 'i', 'in', 'range', '(', '10', ')', ']', 'NEW_LINE', 'x', '.', 'sort', '(', 'reverse', '=', 'True', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '3', ')', ':', 'NEW_LINE', 'INDENT', 'print', '(', 'x', '[', 'i', ']', ')', 'NEW_LINE', 'DEDENT'], 'tgt_id': 'p00001_s854661751'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"valid_file = \"/kaggle/input/pyfixai/valid.jsonl\"\nvalid_data = load_and_preprocess_data(valid_file)\n\nprint(f\"Valid Samples: {len(valid_data)}\")\nprint(valid_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:32:47.839759Z","iopub.execute_input":"2025-04-06T07:32:47.840057Z","iopub.status.idle":"2025-04-06T07:33:02.106645Z","shell.execute_reply.started":"2025-04-06T07:32:47.840034Z","shell.execute_reply":"2025-04-06T07:33:02.105840Z"}},"outputs":[{"name":"stdout","text":"Valid Samples: 20000\n{'src_id': 'p02548_s429693143', 'src': ['N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'K', '=', '0', 'NEW_LINE', 'for', 'C', 'in', 'range', '(', '1', ',', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'K', '+=', 'sum', '(', 'A', '*', 'B', '==', 'N', '-', 'C', 'for', 'A', 'in', 'range', '(', '1', ',', 'N', ')', 'for', 'B', 'in', 'range', '(', '1', ',', 'N', ')', ')', 'NEW_LINE', 'DEDENT', 'print', '(', 'K', ')', 'NEW_LINE'], 'src_verdict': 'Time Limit Exceeded', 'tgt': ['import', 'math', 'NEW_LINE', 'N', '=', 'int', '(', 'input', '(', ')', ')', 'NEW_LINE', 'A', '=', '[', '0', ']', '*', '(', 'N', ')', 'NEW_LINE', 'for', 'i', 'in', 'range', '(', '0', ',', 'N', ')', ':', 'NEW_LINE', 'INDENT', 'A', '[', 'i', ']', '=', 'math', '.', 'floor', '(', '(', 'N', '-', '1', ')', '/', '(', 'i', '+', '1', ')', ')', 'NEW_LINE', 'DEDENT', 'print', '(', 'sum', '(', 'A', ')', ')', 'NEW_LINE'], 'tgt_id': 'p02548_s184073642'}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class CodeFixDataset(Dataset):\n    def __init__(self, data, encoder_tokenizer, decoder_tokenizer, max_length=512):\n        self.data = data\n        self.encoder_tokenizer = encoder_tokenizer\n        self.decoder_tokenizer = decoder_tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        buggy_code = \" \".join(sample['src']).replace(\"NEW_LINE\", \"\\n\").replace(\"INDENT\", \"\").replace(\"DEDENT\", \"\")\n        fixed_code = \" \".join(sample['tgt']).replace(\"NEW_LINE\", \"\\n\").replace(\"INDENT\", \"\").replace(\"DEDENT\", \"\")\n\n        inputs = self.encoder_tokenizer(\n            buggy_code,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        labels = self.decoder_tokenizer(\n            fixed_code,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        labels_input_ids = labels[\"input_ids\"].squeeze()\n        labels_input_ids[labels_input_ids == decoder_tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": labels_input_ids\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:54:17.050995Z","iopub.execute_input":"2025-04-06T14:54:17.051290Z","iopub.status.idle":"2025-04-06T14:54:17.057740Z","shell.execute_reply.started":"2025-04-06T14:54:17.051271Z","shell.execute_reply":"2025-04-06T14:54:17.056856Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"encoder_tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\ndecoder_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# GPT-2 doesn't have a pad token by default\ndecoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n\n# Create datasets and loaders\ntrain_dataset = CodeFixDataset(train_data, encoder_tokenizer, decoder_tokenizer)\nval_dataset = CodeFixDataset(valid_data, encoder_tokenizer, decoder_tokenizer)\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n\nprint(f\"Train Batches: {len(train_loader)}, Validation Batches: {len(val_loader)}\")\n\n# Load encoder-decoder model\nmodel = EncoderDecoderModel.from_encoder_decoder_pretrained(\n    \"microsoft/codebert-base\", \"gpt2\"\n)\n\nmodel.config.decoder_start_token_id = decoder_tokenizer.bos_token_id\nmodel.config.pad_token_id = decoder_tokenizer.pad_token_id\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:33:11.364349Z","iopub.execute_input":"2025-04-06T07:33:11.364654Z","iopub.status.idle":"2025-04-06T07:33:23.073183Z","shell.execute_reply.started":"2025-04-06T07:33:11.364631Z","shell.execute_reply":"2025-04-06T07:33:23.072210Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f97556bb74474281e3bdaded2adb1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aa6d5fb740f450d9800baa16f2c5129"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd8019ff5874b4dbdcec0370fe76a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"990b8ae5aa984ecb9ba14cc4a4ce3bbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d61ca3413948ab9b065f370b5dae6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbd076601a094bf7b050a567394bfbb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5516600c383475ab3a67f8dac8f2af3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f80c84694b4c9091400c7d926cf9be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94a15f18fc5d4706a13e4bcf83a5795f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff537ea1357e4d05a4b3c4e8b3a186b2"}},"metadata":{}},{"name":"stdout","text":"Train Batches: 5000, Validation Batches: 5000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b3c9ff58e6e4dc08af3d142e157d7d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76614a972a5b4bbdab316f3cb844a6ce"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a42c19bd27aa446d832126ac8e51d0fe"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"EncoderDecoderModel(\n  (encoder): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): RobertaPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (decoder): GPT2LMHeadModel(\n    (transformer): GPT2Model(\n      (wte): Embedding(50257, 768)\n      (wpe): Embedding(1024, 768)\n      (drop): Dropout(p=0.1, inplace=False)\n      (h): ModuleList(\n        (0-11): 12 x GPT2Block(\n          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (attn): GPT2SdpaAttention(\n            (c_attn): Conv1D(nf=2304, nx=768)\n            (c_proj): Conv1D(nf=768, nx=768)\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (crossattention): GPT2SdpaAttention(\n            (c_attn): Conv1D(nf=1536, nx=768)\n            (q_attn): Conv1D(nf=768, nx=768)\n            (c_proj): Conv1D(nf=768, nx=768)\n            (attn_dropout): Dropout(p=0.1, inplace=False)\n            (resid_dropout): Dropout(p=0.1, inplace=False)\n          )\n          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): GPT2MLP(\n            (c_fc): Conv1D(nf=3072, nx=768)\n            (c_proj): Conv1D(nf=768, nx=3072)\n            (act): NewGELUActivation()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# Optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\nbest_val_loss = float(\"inf\")\n\n# Training loop\nfor epoch in range(5):\n    print(f\"Starting Epoch: {epoch+1}\")\n    model.train()\n    train_loss = 0\n\n    for i, batch in enumerate(train_loader):\n        if i%500 == 0:\n            print(i)\n            \n        optimizer.zero_grad()\n\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        train_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    avg_train_loss = train_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for i, batch in enumerate(val_loader):\n            if i%500 == 0:\n                print(i)\n                \n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n            val_loss += outputs.loss.item()\n\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Epoch {epoch+1}, Validation Loss: {avg_val_loss:.4f}\")\n\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        model.save_pretrained(\"best_codebert_gpt2_model\")\n        encoder_tokenizer.save_pretrained(\"best_codebert_gpt2_model\")\n        decoder_tokenizer.save_pretrained(\"best_codebert_gpt2_model\")\n        print(\"Saved new best model!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T07:33:30.269937Z","iopub.execute_input":"2025-04-06T07:33:30.270253Z","iopub.status.idle":"2025-04-06T13:30:33.166911Z","shell.execute_reply.started":"2025-04-06T07:33:30.270227Z","shell.execute_reply":"2025-04-06T13:30:33.166144Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Starting Epoch: 1\n0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:629: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:649: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 1, Train Loss: 0.7695\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 1, Validation Loss: 1.1680\nSaved new best model!\nStarting Epoch: 2\n0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:629: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:649: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 2, Train Loss: 0.4079\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 2, Validation Loss: 1.1628\nSaved new best model!\nStarting Epoch: 3\n0\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:629: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n  decoder_attention_mask = decoder_input_ids.new_tensor(decoder_input_ids != self.config.pad_token_id)\n/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:649: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n","output_type":"stream"},{"name":"stdout","text":"500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 3, Train Loss: 0.2793\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 3, Validation Loss: 1.2014\nStarting Epoch: 4\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 4, Train Loss: 0.2046\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 4, Validation Loss: 1.2364\nStarting Epoch: 5\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 5, Train Loss: 0.1562\n0\n500\n1000\n1500\n2000\n2500\n3000\n3500\n4000\n4500\nEpoch 5, Validation Loss: 1.2736\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!zip -r /kaggle/working/output_folder.zip /kaggle/working/best_codebert_gpt2_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:39:01.327617Z","iopub.execute_input":"2025-04-06T13:39:01.328003Z","iopub.status.idle":"2025-04-06T13:39:58.014857Z","shell.execute_reply.started":"2025-04-06T13:39:01.327970Z","shell.execute_reply":"2025-04-06T13:39:58.013922Z"}},"outputs":[{"name":"stdout","text":"updating: kaggle/working/best_codebert_gpt2_model/ (stored 0%)\nupdating: kaggle/working/best_codebert_gpt2_model/merges.txt (deflated 53%)\nupdating: kaggle/working/best_codebert_gpt2_model/generation_config.json (deflated 24%)\nupdating: kaggle/working/best_codebert_gpt2_model/vocab.json (deflated 68%)\nupdating: kaggle/working/best_codebert_gpt2_model/model.safetensors (deflated 7%)\nupdating: kaggle/working/best_codebert_gpt2_model/config.json (deflated 76%)\nupdating: kaggle/working/best_codebert_gpt2_model/tokenizer_config.json (deflated 56%)\nupdating: kaggle/working/best_codebert_gpt2_model/special_tokens_map.json (deflated 74%)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!ls -lh /kaggle/working/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T13:41:59.452616Z","iopub.execute_input":"2025-04-06T13:41:59.453008Z","iopub.status.idle":"2025-04-06T13:41:59.668610Z","shell.execute_reply.started":"2025-04-06T13:41:59.452977Z","shell.execute_reply":"2025-04-06T13:41:59.667508Z"}},"outputs":[{"name":"stdout","text":"total 984M\ndrwxr-xr-x 2 root root 4.0K Apr  6 08:45 best_codebert_gpt2_model\n-rw-r--r-- 1 root root 984M Apr  6 13:39 output_folder.zip\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import json\nimport requests\nfrom google.colab import auth  # works in Kaggle too\nimport google.auth\nfrom google.auth.transport.requests import Request\n\n\n\n\nauth.authenticate_user()\ncreds, _ = google.auth.default()\ncreds.refresh(Request())\naccess_token = creds.token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:10:39.058629Z","iopub.execute_input":"2025-04-06T14:10:39.059078Z","iopub.status.idle":"2025-04-06T14:11:25.812032Z","shell.execute_reply.started":"2025-04-06T14:10:39.059043Z","shell.execute_reply":"2025-04-06T14:11:25.811177Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"file_path = \"/kaggle/working/output_folder.zip\"  # Change this\nfile_name = \"output_folder.zip\"\n\nheaders = {\n    \"Authorization\": f\"Bearer {access_token}\"\n}\n\nmetadata = {\n    \"name\": file_name,\n    \"mimeType\": \"application/zip\"\n}\n\nfiles = {\n    \"data\": (\"metadata\", json.dumps(metadata), \"application/json\"),\n    \"file\": open(file_path, \"rb\")\n}\n\nupload_url = \"https://www.googleapis.com/upload/drive/v3/files?uploadType=multipart\"\n\nres = requests.post(upload_url, headers=headers, files=files)\nres.raise_for_status()\n\nprint(\"✅ Upload successful!\")\nprint(\"📁 File ID:\", res.json()[\"id\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:20:43.194350Z","iopub.execute_input":"2025-04-06T14:20:43.194728Z","iopub.status.idle":"2025-04-06T14:20:56.019429Z","shell.execute_reply.started":"2025-04-06T14:20:43.194696Z","shell.execute_reply":"2025-04-06T14:20:56.018569Z"}},"outputs":[{"name":"stdout","text":"✅ Upload successful!\n📁 File ID: 1yCQVs_tisx7_SmBftMte6MkxexQ5FDIG\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"def load_and_preprocess_data(file_path, limit=1):\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n    return data[:limit]\n\ntest_data = load_and_preprocess_data(\"/kaggle/input/pyfix-test/test.jsonl\", limit=1)\nencoder_tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\ndecoder_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# GPT-2 doesn't have a pad token by default\ndecoder_tokenizer.pad_token = decoder_tokenizer.eos_token\n\n# Create datasets and loaders\ntest_dataset = CodeFixDataset(test_data, encoder_tokenizer, decoder_tokenizer)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n\nmodel = EncoderDecoderModel.from_pretrained(\"/kaggle/working/best_codebert_gpt2_model\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nmodel.eval()\n\ndef test_model(input_code: str):\n    inputs = encoder_tokenizer(input_code, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n    \n    with torch.no_grad():\n        output_ids = model.generate(\n            **inputs,\n            max_length=128,\n            num_beams=4,\n            early_stopping=True\n        )\n    \n    decoded_output = decoder_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return decoded_output\n\ntest_snippet = test_data[0]['src']\nfixed_code = test_model(test_snippet)\nprint(\"Fixed code:\", fixed_code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T14:54:26.247039Z","iopub.execute_input":"2025-04-06T14:54:26.247315Z","iopub.status.idle":"2025-04-06T14:54:41.881011Z","shell.execute_reply.started":"2025-04-06T14:54:26.247296Z","shell.execute_reply":"2025-04-06T14:54:41.879683Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0marg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"repo_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"from_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"to_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrepo_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         raise HFValidationError(\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;34m\"Repo id must be in the form 'repo_name' or 'namespace/repo_name':\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/kaggle/working/best_codebert_gpt2_model'. Use `repo_type` argument if needed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-faf31ae20145>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoderModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/kaggle/working/best_codebert_gpt2_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_fast_init\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3516\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3517\u001b[0m                 \u001b[0;31m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3518\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   3519\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3520\u001b[0m                     \u001b[0mCONFIG_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There was a specific connection error when trying to load {path_or_repo_id}:\\n{err}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHFValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;34mf\"Incorrect path_or_model_id: '{path_or_repo_id}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m         ) from e\n","\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: '/kaggle/working/best_codebert_gpt2_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."],"ename":"OSError","evalue":"Incorrect path_or_model_id: '/kaggle/working/best_codebert_gpt2_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.","output_type":"error"}],"execution_count":4}]}