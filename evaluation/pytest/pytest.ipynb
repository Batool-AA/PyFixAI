{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, RobertaTokenizer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import pytest\n",
    "import sys\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the test dataset (expected target outputs)\n",
    "# with open(\"../../dataset/data/python/processed_with_verdict/test.jsonl\", \"r\") as file:\n",
    "#     test_data = [json.loads(line) for i, line in enumerate(file) if i < 10]\n",
    "\n",
    "# Load the test dataset (expected target outputs)\n",
    "with open(\"../sample_test_set.jsonl\", \"r\") as file:\n",
    "    test_data = [json.loads(line) for i, line in enumerate(file)]\n",
    "test_data = test_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Token to code converter\n",
    "def detokenize(tokens):\n",
    "    code = \"\"\n",
    "    indent_level = 0\n",
    "    for token in tokens:\n",
    "        if token == \"NEW_LINE\":\n",
    "            code += \"\\n\" + (\"    \" * indent_level)\n",
    "        elif token == \"INDENT\":\n",
    "            indent_level += 1\n",
    "            code += \"\\n\" + (\"    \" * indent_level)\n",
    "        elif token == \"DEDENT\":\n",
    "            indent_level -= 1\n",
    "            code += \"\\n\" + (\"    \" * indent_level)\n",
    "        else:\n",
    "            if code and code[-1] not in (\"\\n\", \" \", \"(\", \"[\", \"{\"):\n",
    "                code += \" \"\n",
    "            code += token\n",
    "    return code.strip()\n",
    "\n",
    "# Code runner\n",
    "def run_python_code(code: str, input_data: str):\n",
    "    old_stdout, old_stdin = sys.stdout, sys.stdin\n",
    "    sys.stdout = output = StringIO()\n",
    "    sys.stdin = StringIO(input_data)\n",
    "    try:\n",
    "        exec(code, {})\n",
    "        return output.getvalue().strip()\n",
    "    except Exception as e:\n",
    "        return f\"__ERROR__::{str(e)}\"\n",
    "    finally:\n",
    "        sys.stdout = old_stdout\n",
    "        sys.stdin = old_stdin\n",
    "        \n",
    "\n",
    "def test_model_code_matches_target(model_tokens, target_tokens, input_data, decode=False):\n",
    "    if decode:\n",
    "        model_code = detokenize(model_tokens)\n",
    "    else:\n",
    "        model_code = model_tokens\n",
    "\n",
    "    target_code = detokenize(target_tokens)\n",
    "    model_output = run_python_code(model_code, input_data)\n",
    "    target_output = run_python_code(target_code, input_data)\n",
    "\n",
    "    assert model_output == target_output, f\"Model: {model_output} | Target: {target_output}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def evaluate_seq2seq(model, tokenizer):\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    for entry in test_data:\n",
    "        src_code = entry[\"src\"]\n",
    "        expected_output = entry[\"tgt\"]\n",
    "\n",
    "        # Tokenize input with attention_mask explicitly passed\n",
    "        inputs = tokenizer(\n",
    "            src_code,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128\n",
    "            )\n",
    "\n",
    "        predicted_code = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(predicted_code)\n",
    "\n",
    "        predictions.append(predicted_code)\n",
    "        ground_truths.append(expected_output)\n",
    "\n",
    "        try:\n",
    "            test_model_code_matches_target(predicted_code, expected_output, \"3\", decode=False)\n",
    "        except AssertionError as e:\n",
    "            assert \"Model\" in str(e) and \"Target\" in str(e)\n",
    "\n",
    "    return \"Passed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, d_model)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class CodeErrorFixModel(nn.Module):\n",
    "    def __init__(self, encoder_model_name, vocab_size, embed_size=768, num_decoder_layers=6, nhead=8):\n",
    "        super().__init__()\n",
    "        # Load the pretrained CodeBERT encoder\n",
    "        self.encoder = AutoModel.from_pretrained(encoder_model_name)\n",
    "        # Decoder components\n",
    "        self.decoder_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_encoder = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dropout=0.1)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        # Create a mask to ensure that each position only attends to previous positions\n",
    "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "        return mask.to(next(self.parameters()).device)\n",
    "    \n",
    "    def forward(self, src_input_ids, src_attention_mask, tgt_input_ids, tgt_attention_mask):\n",
    "        # Encode source sequence\n",
    "        encoder_outputs = self.encoder(input_ids=src_input_ids, attention_mask=src_attention_mask)\n",
    "        memory = encoder_outputs.last_hidden_state  # shape: (batch_size, src_seq_len, embed_size)\n",
    "        \n",
    "        # Prepare target embeddings\n",
    "        tgt_embeddings = self.decoder_embedding(tgt_input_ids) * math.sqrt(self.embed_size)\n",
    "        tgt_embeddings = self.pos_encoder(tgt_embeddings)\n",
    "        # Transformer expects (seq_len, batch_size, embed_size)\n",
    "        tgt_embeddings = tgt_embeddings.transpose(0, 1)\n",
    "        memory = memory.transpose(0, 1)\n",
    "        \n",
    "        tgt_seq_len = tgt_input_ids.size(1)\n",
    "        # Create target mask for auto-regressive generation\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len)\n",
    "        \n",
    "        decoder_output = self.decoder(tgt=tgt_embeddings, memory=memory, tgt_mask=tgt_mask)\n",
    "        # Transpose back: (batch_size, seq_len, embed_size)\n",
    "        decoder_output = decoder_output.transpose(0, 1)\n",
    "        logits = self.fc_out(decoder_output)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n",
    "\n",
    "def encode_example(example, tokenizer, max_length=512):\n",
    "    src_tokens = example['src']\n",
    "    tgt_tokens = example['tgt']\n",
    "    src_text = \" \".join(src_tokens)\n",
    "    \n",
    "    # Add start and end tokens to the target\n",
    "    tgt_text = \"<s> \" + \" \".join(tgt_tokens) + \" </s>\"\n",
    "    \n",
    "    src_enc = tokenizer(src_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    tgt_enc = tokenizer(tgt_text, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    return src_enc, tgt_enc\n",
    "\n",
    "def pre_tokenize_data(data, tokenizer, max_length=512):\n",
    "    tokenized_data = []\n",
    "    src_enc, tgt_enc = encode_example(data, tokenizer, max_length)\n",
    "    tokenized_data.append({\n",
    "        'src_input_ids': src_enc['input_ids'].squeeze(0),\n",
    "        'src_attention_mask': src_enc['attention_mask'].squeeze(0),\n",
    "        'tgt_input_ids': tgt_enc['input_ids'].squeeze(0),\n",
    "        'tgt_attention_mask': tgt_enc['attention_mask'].squeeze(0)\n",
    "    })\n",
    "    return tokenized_data\n",
    "\n",
    "def evaluate_custom(model, tokenizer, device='cpu'):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "    ground_truths = []\n",
    "\n",
    "    for entry in test_data:\n",
    "        tokenized_test_data = pre_tokenize_data(entry, tokenizer, max_length=512)\n",
    "        test_loader = DataLoader(tokenized_test_data, batch_size=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                src_input_ids = batch['src_input_ids'].to(device)\n",
    "                src_attention_mask = batch['src_attention_mask'].to(device)\n",
    "                tgt_input_ids = batch['tgt_input_ids'].to(device)\n",
    "                tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n",
    "\n",
    "                # ðŸ”® Predict (use teacher forcing)\n",
    "                output = model(src_input_ids, src_attention_mask, tgt_input_ids[:, :-1], tgt_attention_mask[:, :-1])\n",
    "                predicted_ids = output.argmax(dim=-1)\n",
    "\n",
    "                # Decode Input (buggy), Prediction, and Target (ground truth)\n",
    "                input_text = tokenizer.decode(src_input_ids[0], skip_special_tokens=True)\n",
    "                predicted_text = tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "                target_text = tokenizer.decode(tgt_input_ids[0], skip_special_tokens=True)\n",
    "        print(predicted_text)\n",
    "        predictions.append(predicted_text)\n",
    "        ground_truths.append(target_text)\n",
    "\n",
    "        try:\n",
    "            test_model_code_matches_target(predicted_text, target_text, \"3\", decode=True)\n",
    "        except AssertionError as e:\n",
    "            assert \"Model\" in str(e) and \"Target\" in str(e)\n",
    "        \n",
    "    return \"Passed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "\n",
    "model_path = \"Salesforce/codeT5-base\"  # Path to your saved model folder\n",
    "print(\"----------------------- CodeT5 --------------------\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "codeT5_result = evaluate_seq2seq(model, tokenizer)\n",
    "print(f\"Results: {codeT5_result}\")  \n",
    "\n",
    "\n",
    "model_path = \"../../codebert-gpt2\"  # Path to your saved model folder\n",
    "print(\"----------------------- CodeBERT-GPT2 --------------------\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "codeBERTGPT_result = evaluate_seq2seq(model, tokenizer)\n",
    "print(f\"Average BLEU Score: {codeBERTGPT_result}\")\n",
    "\n",
    "\n",
    "model_path = \"../../codebert-codebert\"  # Path to your saved model folder\n",
    "print(\"----------------------- CodeBERT-CodeBERT --------------------\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "codeBERTBERT_result = evaluate_seq2seq(model, tokenizer)\n",
    "print(f\"Average BLEU Score: {codeBERTBERT_result}\")\n",
    "\n",
    "\n",
    "model_path = \"../../codebert-custom/full_model.pth\"  # Path to your saved model folder\n",
    "print(\"----------------------- CodeBERT-Custom --------------------\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "model = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "codeBERTcustom_result = evaluate_custom(model, tokenizer)\n",
    "print(f\"Average BLEU Score: {codeBERTcustom_result}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
